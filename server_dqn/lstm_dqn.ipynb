{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import math\n",
    "import threading\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_excel('nov_nine_var.xlsx', engine='openpyxl')[0:430].to_numpy()\n",
    "goal_data = pd.read_excel('basic_formula.xlsx', engine='openpyxl').to_numpy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(real_data[:,1:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(l):\n",
    "    return max(range(len(l)), key=lambda i: l[i])\n",
    "\n",
    "def argmin(l):\n",
    "    return min(range(len(l)), key=lambda i: l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.508362 0.497095 0.492966 0.494638 0.502046 0.500935 0.502494 0.500844\n",
      " 0.504019 0.502051 0.494209 0.498915 0.504325 0.500418 0.500487 0.49806\n",
      " 0.496443 0.494332 0.511553 0.498666 0.497106], shape=(21,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[0.       0.982134 0.       1.       0.991927 0.996404 0.018404 0.736051\n",
      " 0.039968 0.378589 0.342857 0.578947 0.25     0.320856 0.223881 0.223404\n",
      " 1.       0.015124 0.496289 0.564583 1.      ], shape=(21,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "start = tf.constant(scaler.transform(real_data[:,1:22])[-1].reshape(1, 21), dtype=tf.float32)\n",
    "goal = tf.constant(scaler.transform(goal_data[:,1:22])[argmin(goal_data[:,-1])].reshape(1, 21), dtype=tf.float32)\n",
    "\n",
    "print(goal[0])\n",
    "print(start[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = tf.constant(scaler.transform(real_data[:,1:22])[-61 : -1].reshape(1, 60, 21), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramater\n",
    "ACTION_NUM = 5\n",
    "LENTGTH = 60\n",
    "EPISODE_DONE = 1000\n",
    "\n",
    "# learn paramater\n",
    "TRAIN_FLAG = EPISODE_DONE * 10\n",
    "LEARN = 1000\n",
    "\n",
    "MEMORY_SIZE = EPISODE_DONE * LEARN\n",
    "\n",
    "# dqn paramater\n",
    "GAMMA = 0.999\n",
    "EPS_DECAY = 0.0005\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = input(\"one or all?: \") == 'one'\n",
    "m = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if m:\n",
    "    model_list = [\n",
    "        [\n",
    "            tf.keras.models.load_model('./model/one_lstm/one_lstm_{0}/{1}_model'.format(j, i)) for i in range(21)\n",
    "        ]   for j in range(ACTION_NUM)\n",
    "    ]\n",
    "\n",
    "else:\n",
    "    model_list = [\n",
    "        tf.keras.models.load_model('./model/action_net/action_net{0}'.format(i)) for i in range(5)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_data(origin, d, one):\n",
    "    if one:\n",
    "        shift_d = np.zeros((1, LENTGTH, 21))\n",
    "        for i in range(21):\n",
    "            d_s = d[:,i]\n",
    "            shift_d[0][:,i] = np.concatenate((origin[0][1::][:,i], d_s), axis=0).reshape(1, LENTGTH)\n",
    "    \n",
    "    else:\n",
    "        shift_d = tf.concat((origin[0][1::], d), axis=0).reshape(1, LENTGTH, 21)\n",
    "    return shift_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_action(idx, s, one):\n",
    "    if one:\n",
    "        model_pred = np.zeros((5, 21, 1))\n",
    "        for i in range(5):\n",
    "            for j in range(21):\n",
    "                s_s = s[:,:,j].reshape(1, LENTGTH, 1)\n",
    "                model_pred[i][j] = model_list[i][j](s_s)[0]\n",
    "        \n",
    "        return model_pred[idx].T\n",
    "\n",
    "    else:\n",
    "        return model_list[idx](s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(s, a):\n",
    "    ns = a\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(ns, gs):\n",
    "    ns_s = ns[:,0:21]\n",
    "    dist = -tf.math.sqrt(tf.math.reduce_sum(tf.math.square(gs - ns_s)))\n",
    "\n",
    "    end = 100\n",
    "    equal = tf.math.count_nonzero(ns_s-gs) * -4\n",
    "\n",
    "    reward = end + equal + dist\n",
    "    return tf.cast(reward, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.8\n",
    "    beta = 0.3\n",
    "    beta_increment_per_sampling = 0.0005\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (tf.math.abs(error) + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = tf.cast(self._get_priority(error), dtype=tf.float32)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        a_is = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        priorities = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "\n",
    "        state_hist = tf.constant(init_state)\n",
    "        action_hist = tf.constant([[0]], dtype=tf.int32)\n",
    "        reward_hist = tf.constant([[0]], dtype=tf.float32)\n",
    "        next_hist = tf.constant(init_state, dtype=tf.float32)\n",
    "        done_hist = tf.constant([[0]], dtype=tf.int32)\n",
    "        \n",
    "        segment = tf.cast(self.tree.total() / n, tf.float32)\n",
    "\n",
    "        self.beta = tf.cast(tf.math.minimum(1., self.beta + self.beta_increment_per_sampling), dtype=tf.float32)\n",
    "\n",
    "        for i in tf.range(n):\n",
    "            a = tf.cast(segment * i, tf.float32)\n",
    "            b = tf.cast(segment * (i + 1), tf.float32)\n",
    "\n",
    "            s = tf.random.uniform(shape=(), minval=a, maxval=b, dtype=tf.float32)\n",
    "            a_i_p, data = self.tree.get(s)\n",
    "\n",
    "            priorities = priorities.write(i, a_i_p[1])\n",
    "            a_is = a_is.write(i, a_i_p[0])\n",
    "\n",
    "            state_hist = tf.concat((state_hist, data[0]), axis=0)\n",
    "            action_hist = tf.concat((action_hist, data[1]), axis=0)\n",
    "            reward_hist = tf.concat((reward_hist, data[2]), axis=0)\n",
    "            next_hist = tf.concat((next_hist, data[3]), axis=0)\n",
    "            done_hist = tf.concat((done_hist, data[4]), axis=0)\n",
    "            \n",
    "        priorities = priorities.stack()\n",
    "        a_is = a_is.stack()\n",
    "\n",
    "        sampling_probabilities = tf.cast(priorities / self.tree.total(), dtype=tf.float32)\n",
    "        \n",
    "        is_weight = tf.math.pow(tf.math.multiply(self.tree.n_entries, sampling_probabilities), -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "        \n",
    "        batch = (state_hist[1::], action_hist[1::], reward_hist[1::], next_hist[1::], done_hist[1::])\n",
    "        return batch, a_is, is_weight\n",
    "\n",
    "    def update(self, a_i, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(a_i, p)\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = tf.Variable(tf.zeros(2 * capacity - 1, dtype=tf.float32))\n",
    "\n",
    "        self.state_hist = tf.Variable(tf.zeros((capacity, 42), dtype=tf.float32))\n",
    "        self.action_hist = tf.Variable(tf.zeros((capacity, 1), dtype=tf.int32))\n",
    "        self.reward_hist = tf.Variable(tf.zeros((capacity, 1), dtype=tf.float32))\n",
    "        self.next_hist = tf.Variable(tf.zeros((capacity, 42), dtype=tf.float32))\n",
    "        self.done_hist = tf.Variable(tf.zeros((capacity, 1), dtype=tf.int32))\n",
    "\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, a_i, change):\n",
    "        parent = (a_i - 1) // 2\n",
    "\n",
    "        self.tree[parent].assign(self.tree[parent]+change)\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, a_i, s):\n",
    "        left = 2 * a_i + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= self.tree.shape[0]:\n",
    "            return a_i\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree.shape[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        a_i = self.write + self.capacity - 1\n",
    "\n",
    "        self.state_hist[self.write].assign(data[0])\n",
    "        self.action_hist[self.write].assign(data[1])\n",
    "        self.reward_hist[self.write].assign(data[2])\n",
    "        self.next_hist[self.write].assign(data[3])\n",
    "        self.done_hist[self.write].assign(data[4])\n",
    "        \n",
    "        self.update(a_i, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, a_i, p):\n",
    "        change = p - self.tree[a_i]\n",
    "\n",
    "        self.tree[a_i].assign(p)\n",
    "        self._propagate(a_i, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        a_i = self._retrieve(0, s)\n",
    "        dataa_i = a_i - self.capacity + 1\n",
    "        \n",
    "        data = (self.state_hist[dataa_i].reshape(1, 42), self.action_hist[dataa_i].reshape(1, 1), self.reward_hist[dataa_i].reshape(1, 1), self.next_hist[dataa_i].reshape(1, 42), self.done_hist[dataa_i].reshape(1, 1))\n",
    "        return (a_i, self.tree[a_i]), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Network(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN_Network, self).__init__()\n",
    "        self.input_layer = tf.keras.models.Sequential()\n",
    "        self.input_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.input_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "        self.q_layer = tf.keras.models.Sequential()\n",
    "        self.q_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.q_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.q_layer.add(tf.keras.layers.Dense(1, activation='linear'))\n",
    "\n",
    "        self.adv_layer = tf.keras.models.Sequential()\n",
    "        self.adv_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.adv_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.adv_layer.add(tf.keras.layers.Dense(ACTION_NUM, activation='linear'))\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        i = self.input_layer(x)\n",
    "\n",
    "        q = self.q_layer(i)\n",
    "        adv = self.adv_layer(i)\n",
    "\n",
    "        o = q + adv - tf.math.reduce_mean(adv, axis=1, keepdims=True)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self):\n",
    "        self.train_model = self.set_model()\n",
    "        self.target_model = self.set_model()\n",
    "        self.target_model.trainable = False\n",
    "\n",
    "        self.memory = Memory(MEMORY_SIZE)\n",
    "        self.episode = 1\n",
    "        self.eps_threshold = 1\n",
    "\n",
    "        self.optim = tf.keras.optimizers.RMSprop(learning_rate=1e-6)\n",
    "\n",
    "    def set_model(self):\n",
    "        net = DQN_Network()\n",
    "        net.build(input_shape=(1, 42))\n",
    "\n",
    "        optim = tf.keras.optimizers.RMSprop(learning_rate=1e-6)\n",
    "        net.compile(optimizer=optim, loss='mse')\n",
    "        return net\n",
    "\n",
    "    def update_model(self):\n",
    "        self.target_model.set_weights(self.train_model.get_weights())\n",
    "\n",
    "    def soft_update_model(self, tau=0.1):\n",
    "        train_weight = np.array(self.train_model.get_weights(), dtype=object)\n",
    "        target_weight = np.array(self.target_model.get_weights(), dtype=object)\n",
    "\n",
    "        weight = train_weight * tau + target_weight * (1-tau)\n",
    "        self.target_model.set_weights(weight)\n",
    "\n",
    "    def memorize(self, cs, a_i, r, ns, d):\n",
    "        if d and self.memory.tree.n_entries > TRAIN_FLAG:\n",
    "            self.episode += 1\n",
    "\n",
    "        td_error = r + GAMMA * tf.argmax(self.target_model(ns)[0]) - tf.argmax(self.train_model(cs)[0])\n",
    "        data = (cs, a_i, r, ns, d)\n",
    "        self.memory.add(td_error, data)\n",
    "\n",
    "    def convert_memory_to_input(self, batch):\n",
    "        s, a_i, r, ns, d = zip(batch)\n",
    "\n",
    "        states = tf.convert_to_tensor(s).reshape(BATCH_SIZE, 42)\n",
    "        action_indexs = tf.convert_to_tensor(a_i).reshape(BATCH_SIZE, 1)\n",
    "        rewards = tf.convert_to_tensor(r).reshape(BATCH_SIZE, 1)\n",
    "        next_states = tf.convert_to_tensor(ns).reshape(BATCH_SIZE, 42)\n",
    "        dones = tf.convert_to_tensor(d).reshape(BATCH_SIZE, 1)\n",
    "\n",
    "        return states, action_indexs, rewards, next_states, dones\n",
    "\n",
    "    def act(self, state):\n",
    "        a_r = self.train_model(state)[0]\n",
    "\n",
    "        if tf.random.uniform(shape=(), maxval=1, dtype=tf.float32) > self.eps_threshold:\n",
    "            a_i = tf.argmax(a_r)\n",
    "            c = 1\n",
    "\n",
    "        else:\n",
    "            a_i = int(tf.random.uniform(shape=(), maxval=ACTION_NUM-1))\n",
    "            c = 0\n",
    "            \n",
    "        a_i = tf.cast(a_i, dtype=tf.int32)\n",
    "\n",
    "        return a_i, c, self.eps_threshold\n",
    "\n",
    "    def run(self):\n",
    "        if self.memory.tree.n_entries < TRAIN_FLAG:\n",
    "            return 1\n",
    "        \n",
    "        self.eps_threshold = 0.05 + (1 - 0.05) * math.exp(-1. * self.episode * EPS_DECAY)\n",
    "\n",
    "        batch, a_is, is_weight = self.memory.sample(BATCH_SIZE)\n",
    "\n",
    "        states, action_indexs, rewards, next_states, dones = self.convert_memory_to_input(batch)\n",
    "\n",
    "        is_weight = tf.convert_to_tensor(is_weight)\n",
    "        loss = self.learn(states, action_indexs, rewards, next_states, dones, is_weight)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def learn(self, states, action_indexs, rewards, next_states, dones, is_weight):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.train_model.trainable_variables)\n",
    "\n",
    "            q = self.train_model(states)\n",
    "            next_q = self.train_model(next_states)\n",
    "            next_target_q = self.target_model(next_states)\n",
    "\n",
    "            next_action = tf.argmax(next_q, axis=1).reshape(next_q.shape[0], 1)\n",
    "\n",
    "            target_val = tf.reduce_sum(tf.one_hot(next_action, ACTION_NUM) * next_target_q, axis=1)\n",
    "            target_q = rewards + (1 - dones) * GAMMA * target_val\n",
    "\n",
    "            main_val = tf.reduce_sum(tf.one_hot(action_indexs, ACTION_NUM) * q, axis=1)\n",
    "\n",
    "            error = tf.square(main_val - target_val) * 0.5\n",
    "            loss = tf.reduce_mean(error)\n",
    "\n",
    "        grads = tape.gradient(loss, self.train_model.trainable_weights)\n",
    "        grads = [(tf.clip_by_value(grad, -1.0, 1.0)) for grad in grads]\n",
    "        self.optim.apply_gradients(zip(grads, self.train_model.trainable_weights))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_run(agent, m, max_step, max_episode, init_state, init_model_state):\n",
    "    global state_hist, action_hist, reward_hist, next_hist, done_hist\n",
    "    \n",
    "    for e in range(max_episode):\n",
    "        state = init_state\n",
    "        model_state = init_model_state\n",
    "        steps = 1\n",
    "        \n",
    "        while True:\n",
    "            model_state = shift_data(model_state, state[:,0:21], m)\n",
    "            a_i, t, eps = agent.act(state)\n",
    "            action = return_action(a_i, model_state, m)\n",
    "            \n",
    "            checker = state[:,0:21] == goal[0]\n",
    "            if all(checker[0]):\n",
    "                done = 1\n",
    "                b = 1\n",
    "            else:\n",
    "                done = 0\n",
    "                b = 0\n",
    "\n",
    "            next_state = tf.concat((return_state(state, action), goal), axis=1)\n",
    "            reward = return_reward(next_state, goal)\n",
    "\n",
    "            if steps == max_step:\n",
    "                done = 1\n",
    "\n",
    "            else:\n",
    "                bbbb = 0\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "            state_hist = tf.concat((state_hist, state), axis=0)\n",
    "            action_hist = tf.concat((action_hist, tf.constant(a_i).reshape(1, 1)), axis=0)\n",
    "            reward_hist = tf.concat((reward_hist, tf.constant(reward).reshape(1, 1)), axis=0)\n",
    "            next_hist = tf.concat((next_hist, next_state), axis=0)\n",
    "            done_hist = tf.concat((done_hist, tf.constant(done).reshape(1, 1)), axis=0)\n",
    "\n",
    "#             if done and e % 10 == 0:\n",
    "            if done:\n",
    "                print(f'={e}=|{reward}')\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memorize(agent, states, actions, rewards, nexts, dones):\n",
    "    global save_state, save_reward\n",
    "    \n",
    "    save_state = tf.concat((save_state, states), axis=0)\n",
    "    save_reward = tf.concat((save_reward, rewards), axis=0)\n",
    "\n",
    "    for i in range(1, len(states)):\n",
    "        agent.memorize(states[i].reshape(1, 42), actions[i], rewards[i], nexts[i].reshape(1, 42), dones[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=0=|14.288034439086914\n",
      "=1=|14.253286361694336\n",
      "=2=|14.223055839538574\n",
      "=3=|14.269937515258789\n",
      "=4=|14.285579681396484\n",
      "data collection\n",
      "=0=|14.285579681396484\n",
      "=1=|14.223615646362305\n",
      "=2=|14.223055839538574\n",
      "=3=|14.223055839538574\n",
      "=4=|14.285115242004395\n",
      "memorizing\n",
      "run\n",
      "(512, 42) (512, 1) (512, 1) (512, 42) (512, 1)\n",
      "data collection\n",
      "=0=|14.282330513000488\n",
      "=1=|14.252022743225098\n",
      "=2=|14.285579681396484\n",
      "=3=|14.253286361694336\n",
      "=4=|14.223128318786621\n",
      "memorizing\n",
      "run\n",
      "(512, 42) (512, 1) (512, 1) (512, 42) (512, 1)\n",
      "data collection\n",
      "=0=|14.269937515258789\n",
      "=1=|14.285579681396484\n",
      "=2=|14.223055839538574\n",
      "=3=|14.223055839538574\n",
      "=4=|14.288034439086914\n",
      "memorizing\n",
      "run\n",
      "(512, 42) (512, 1) (512, 1) (512, 42) (512, 1)\n",
      "data collection\n",
      "=0=|14.222705841064453\n",
      "=1=|14.253286361694336\n",
      "=2=|14.288034439086914\n",
      "=3=|14.285115242004395\n",
      "=4=|14.222705841064453\n",
      "memorizing\n",
      "run\n",
      "(512, 42) (512, 1) (512, 1) (512, 42) (512, 1)\n",
      "data collection\n",
      "=0=|14.222705841064453\n",
      "=1=|14.268577575683594\n",
      "=2=|14.252462387084961\n",
      "=3=|14.268646240234375\n",
      "=4=|14.223055839538574\n",
      "memorizing\n",
      "run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_Agent()\n",
    "# agent.train_model.set_weights(tf.keras.models.load_model('./model/dqn_net').get_weights())\n",
    "\n",
    "init_state = tf.concat((start, goal), axis=1)\n",
    "init_model_state = model_state\n",
    "\n",
    "global state_hist, action_hist, reward_hist, next_hist, done_hist\n",
    "\n",
    "state_hist = tf.constant(tf.zeros((1, 42), dtype=tf.float32))\n",
    "action_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "reward_hist = tf.constant(tf.zeros((1, 1), dtype=tf.float32))\n",
    "next_hist = tf.constant(tf.zeros((1, 42), dtype=tf.float32))\n",
    "done_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "\n",
    "save_state = state_hist\n",
    "save_reward = reward_hist\n",
    "\n",
    "max_step = EPISODE_DONE\n",
    "max_episode = 5\n",
    "agent.memory.tree.n_entries = TRAIN_FLAG\n",
    "\n",
    "# env_thread = threading.Thread(target=env_run, args=(agent, m, max_step, max_episode, init_state, init_model_state))\n",
    "# env_thread.daemon = True\n",
    "\n",
    "# print(\"data collection\")\n",
    "# env_thread.start()\n",
    "# env_thread.join()\n",
    "\n",
    "env_run(agent, m, max_step, max_episode, init_state, init_model_state)\n",
    "\n",
    "for ti in range(LEARN):\n",
    "#     env_thread = threading.Thread(target=env_run, args=(agent, m, max_step, max_episode, init_state, init_model_state))\n",
    "#     env_thread.daemon = True\n",
    "\n",
    "    print(\"data collection\")\n",
    "#     env_thread.start()\n",
    "\n",
    "    env_run(agent, m, max_step, max_episode, init_state, init_model_state)\n",
    "\n",
    "#     memorize_thread = threading.Thread(target=memorize, args=(agent, state_hist, action_hist, reward_hist, next_hist, done_hist))\n",
    "#     memorize_thread.daemon = True\n",
    "    \n",
    "    print(\"memorizing\")\n",
    "    memorize(agent, state_hist, action_hist, reward_hist, next_hist, done_hist)\n",
    "#     memorize_thread.start()\n",
    "#     memorize_thread.join()\n",
    "\n",
    "    state_hist = tf.constant(init_state)\n",
    "    action_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "    reward_hist = tf.constant(tf.zeros((1, 1), dtype=tf.float32))\n",
    "    next_hist = tf.constant(init_state)\n",
    "    done_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "    \n",
    "    print(\"run\")\n",
    "    loss = agent.run()\n",
    "    agent.soft_update_model()\n",
    "\n",
    "#     env_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n",
       "array([[-0.074353,  0.144177,  0.151431,  0.243113,  0.064228]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train_model(init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1170\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iterable_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m replace_iterable_params(args, kwargs, iterable_params)\n\u001b[0;32m-> 1170\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapi_dispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1172\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: Got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "tf.argmax(agent.train_model(init_state)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agent = DQN_Agent()\n",
    "# # agent.train_model.set_weights(tf.keras.models.load_model('./model/dqn_net').get_weights())\n",
    "\n",
    "# init_state = tf.concat((start, goal), axis=1)\n",
    "\n",
    "# state_hist = tf.constant(init_state)\n",
    "# action_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "# reward_hist = tf.constant(tf.zeros((1, 1), dtype=tf.float32))\n",
    "# next_hist = tf.constant(init_state)\n",
    "# done_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "\n",
    "# save_state = state_hist\n",
    "# save_reward = reward_hist\n",
    "\n",
    "# for e in range(50000 + TRAIN_FLAG // EPISODE_DONE):\n",
    "#     state = init_state\n",
    "#     steps = 1\n",
    "#     rewards = 0\n",
    "#     max_reward = -1e+4\n",
    "\n",
    "#     while True:\n",
    "# #         state_hist.append(state[0][0:21])\n",
    "#         model_state = shift_data(model_state, state[:,0:21], m)\n",
    "#         a_i, t, eps = agent.act(state)\n",
    "#         action = return_action(a_i, model_state, m)\n",
    "\n",
    "#         checker = state[:,0:21] == goal[0]\n",
    "#         if all(checker[0]):\n",
    "#             done = 1\n",
    "#         else:\n",
    "#             done = 0\n",
    "        \n",
    "#         next_state = tf.concat((return_state(state, action), goal), axis=1)\n",
    "#         reward = return_reward(next_state, goal)\n",
    "\n",
    "#         if abs(reward) > 999 or steps == EPISODE_DONE:\n",
    "#             bbbb = 1\n",
    "#             done = 1\n",
    "#         else:\n",
    "#             bbbb = 0\n",
    "#             done = 0\n",
    "\n",
    "#         state = next_state\n",
    "#         rewards += reward\n",
    "#         steps += 1\n",
    "        \n",
    "#         state_hist = tf.concat((state_hist, state), axis=0)\n",
    "#         action_hist = tf.concat((action_hist, tf.constant(a_i).reshape(1, 1)), axis=0)\n",
    "#         reward_hist = tf.concat((reward_hist, tf.constant(reward).reshape(1, 1)), axis=0)\n",
    "#         next_hist = tf.concat((next_hist, next_state), axis=0)\n",
    "#         done_hist = tf.concat((done_hist, tf.constant(done).reshape(1, 1)), axis=0)\n",
    "\n",
    "#         if max_reward < reward:\n",
    "#             max_reward = reward\n",
    "\n",
    "#         if done:\n",
    "#             rewards = rewards if steps - 1 == EPISODE_DONE else -100\n",
    "#             print(f'======{e if bbbb else 0} | {reward}, {max_reward}')\n",
    "#             break\n",
    "        \n",
    "#     if e % MEMORIZE_FREQ == 0:\n",
    "#         save_state = tf.concat((save_state, state_hist), axis=0)\n",
    "#         save_reward = tf.concat((save_reward, reward_hist), axis=0)\n",
    "#         print(\"memorizing\")\n",
    "#         for i in range(1, state_hist.shape[0]):\n",
    "#             agent.memorize(state_hist[i].reshape(1, 42), action_hist[i], reward_hist[i], next_hist[i].reshape(1, 42), done_hist[i])\n",
    "#         state_hist = tf.constant(init_state)\n",
    "#         action_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "#         reward_hist = tf.constant(tf.zeros((1, 1), dtype=tf.float32))\n",
    "#         next_hist = tf.constant(init_state)\n",
    "#         done_hist = tf.constant(tf.zeros((1, 1), dtype=tf.int32))\n",
    "\n",
    "#     if e % LEARN_FREQ == 0:\n",
    "#         print(\"learning\")\n",
    "#         loss = agent.run()\n",
    "#         agent.soft_update_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = tf.keras.models.load_model('./model/dnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dnn_model(state_hist[:,0:21]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
