{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\code\\\\activ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = 'nov_nine_var.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = tf.keras.models.load_model('./model/dnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 128\n",
    "ACTION_NUM = 2\n",
    "EPISODE_DONE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./documents/'+df_name).iloc[:,1:23]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df.iloc[:,0:21])\n",
    "DATUM_STATE = X[-1].reshape(1, 21)\n",
    "DATUM_RATE = df.iloc[:,21:22].iloc[-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_rate_represent_df = pd.read_excel('./documents/other/change_rate_representative.xlsx').iloc[:,1::]\n",
    "FLOOR = change_rate_represent_df[0]\n",
    "TOP = change_rate_represent_df[1]\n",
    "MODE = change_rate_represent_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR = df.corr().iloc[-1].to_numpy()[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(s, a):\n",
    "    ns = s + a.numpy()\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(ns, yp):\n",
    "    dist_loss = np.sqrt(np.sum(np.square(DATUM_STATE - ns)))/21\n",
    "    pop_loss = DATUM_RATE - yp\n",
    "\n",
    "    return -(dist_loss + pop_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_loss(y_true, y_pred):\n",
    "    input_trick = y_true.reshape(32, 22)\n",
    "\n",
    "    reward_loss = input_trick[:,21:22]\n",
    "    next_state = input_trick[:,0:21]\n",
    "    population = dnn_model(next_state)\n",
    "\n",
    "    actions = tf.concat([y_pred[:,0:21], y_pred[:,22:43]], axis=0)\n",
    "\n",
    "    step_loss = 0.0\n",
    "for act in actions:\n",
    "    for i in range(act.shape[0]):\n",
    "        if act[i] < FLOOR[i] or act[i] > TOP[i]:\n",
    "            step_loss += 0.1\n",
    "\n",
    "        step_loss += (act[i] - MODE[i]) ** 2\n",
    "\n",
    "    # ns * x = pp\n",
    "    # ns/pp = 1/x\n",
    "    # x = pp/ns\n",
    "    # corr_loss = (corr-x)^2\n",
    "    \n",
    "    corr_loss = 0\n",
    "    print(next_state.shape)\n",
    "    # for j in range(32):\n",
    "    for i in range(21):\n",
    "        tmp = tf.math.reduce_sum(tf.math.divide(population, next_state[:,i]))\n",
    "        corr_loss += (CORR[i]-tmp) ** 2\n",
    "    \n",
    "    return tf.cast(tf.math.add(step_loss+reward_loss, corr_loss), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_reward_net_input(data_num, s, a, d=False):\n",
    "    input_state = tf.convert_to_tensor([s for i in range(ACTION_NUM)]).reshape(data_num, ACTION_NUM, 21)\n",
    "\n",
    "    if d:\n",
    "        input_action = tf.convert_to_tensor([a for i in range(ACTION_NUM)]).reshape(data_num, ACTION_NUM, 21)\n",
    "    else:\n",
    "        input_action = a.reshape(data_num, ACTION_NUM, 21)\n",
    "\n",
    "    x = tf.convert_to_tensor([input_state, input_action]).reshape(data_num, ACTION_NUM, 2, 21)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    return tf.cast(x, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action_Network(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Action_Network, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.Dense(64, activation='relu')\n",
    "        \n",
    "        self.hidden_layer = tf.keras.models.Sequential()\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "        self.output_layer = tf.keras.models.Sequential()\n",
    "        self.output_layer.add(tf.keras.layers.Dense(ACTION_NUM*22, activation='linear'))\n",
    "\n",
    "    def call(self, x):\n",
    "        # print(\"a: x: \", x.shape)\n",
    "        i = self.input_layer(x)\n",
    "        # print(\"a: i: \", i.shape)\n",
    "        h = self.hidden_layer(i)\n",
    "        # print(\"a: h: \", h.shape)\n",
    "        o = self.output_layer(h)\n",
    "        # print(\"a: o: \", o.shape)\n",
    "\n",
    "        return tf.cast(o, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward_Network(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(Reward_Network, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.Dense(64, activation='relu')\n",
    "\n",
    "        self.hidden_layer = tf.keras.models.Sequential()\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "\n",
    "        self.output_layer = tf.keras.models.Sequential()\n",
    "        self.output_layer.add(tf.keras.layers.Dense(ACTION_NUM, activation='linear'))\n",
    "\n",
    "    def call(self, x):\n",
    "        # print(\"r: x: \", x.shape)\n",
    "        i = self.input_layer(x)\n",
    "        # print(\"r: i: \", i.shape)\n",
    "        h = self.hidden_layer(i)\n",
    "        # print(\"r: h: \", h.shape)\n",
    "        o = self.output_layer(h)\n",
    "        # print(\"r: o: \", o.shape)\n",
    "\n",
    "        return tf.cast(o, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Network(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN_Network, self).__init__()\n",
    "        self.action_network = Action_Network()\n",
    "        self.action_network.build(input_shape=(None, 21))\n",
    "        self.action_network.compile(optimizer='adam', loss=action_loss)\n",
    "\n",
    "        self.reward_network = Reward_Network()\n",
    "        self.reward_network.build(input_shape=(None, ACTION_NUM*2*21))\n",
    "        self.reward_network.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    def call(self, x):\n",
    "        action = self.action_network(x)\n",
    "        input_data = convert_reward_net_input(x.shape[0], x, action[:,0:21], d=True)\n",
    "        reward = self.reward_network(input_data)\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self):\n",
    "        self.train_model = DQN_Network()\n",
    "        # self.train_model.build(input_shape=(None, 21))\n",
    "        self.train_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        self.target_model = DQN_Network()\n",
    "        # self.target_model.build(input_shape=(None, 21))\n",
    "        self.target_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.episode = 1\n",
    "        self.max_reward_actions = {}\n",
    "\n",
    "    def memorize(self, cs, a, a_i, r, ns, d):\n",
    "        \"\"\"\n",
    "            cs: 현재 상태\n",
    "            a: 현재 행동\n",
    "            a_i: 현재 행동 번호(몇번 행동)\n",
    "            r: 현재 행동에 대한 보상\n",
    "            ns: 현재 상황에서 한 행동으로 인한 결과, 다음 상태\n",
    "            s: 에피소드 진행 상태\n",
    "        \"\"\"\n",
    "\n",
    "        done = d\n",
    "        if done:\n",
    "            self.episode += 1\n",
    "\n",
    "        self.memory.append(\n",
    "            (\n",
    "                tf.convert_to_tensor(tf.cast(cs, tf.float32)),\n",
    "                a,\n",
    "                a_i,\n",
    "                tf.convert_to_tensor(r),\n",
    "                tf.convert_to_tensor(tf.cast(ns, tf.float32)),\n",
    "                done\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try: # 현재 상황에서 최대의 보상을 주는 행동을 저장\n",
    "            if self.max_reward_actions[tuple(map(tuple, cs))][1] < r:\n",
    "                self.max_reward_actions[tuple(map(tuple, cs))] = [a, r]\n",
    "        except KeyError:\n",
    "            self.max_reward_actions[tuple(map(tuple, cs))] = [a, r]\n",
    "\n",
    "    def convert_memory(self, length):\n",
    "        batch = rand.sample(self.memory, length)\n",
    "        s, a, a_i, r, ns, d = zip(*batch)\n",
    "\n",
    "        states = tf.convert_to_tensor(s).reshape(length, 21)\n",
    "        actions = tf.convert_to_tensor(a).reshape(length, 21)\n",
    "        action_indexs = tf.convert_to_tensor(a_i)\n",
    "        rewards = tf.convert_to_tensor(r)\n",
    "        next_states = tf.convert_to_tensor(ns).reshape(length, 21)\n",
    "        dones = tf.convert_to_tensor(d)\n",
    "\n",
    "        # print(states.dtype, actions.dtype, action_indexs.dtype, rewards.dtype, next_states.dtype, dones.dtype)\n",
    "\n",
    "        return states, actions, action_indexs, rewards, next_states, dones\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.episode >= 0 and self.episode < 200:\n",
    "            eps_threshold = -(self.episode/1000)+1+(self.episode)*(self.episode-200)/300000\n",
    "        else:\n",
    "            eps_threshold = -(self.episode/1000)+1+(self.episode-200)*(self.episode-1000)\n",
    "\n",
    "        state = tf.cast(tf.convert_to_tensor(state), tf.float32)\n",
    "        action = self.train_model.action_network(state).reshape(ACTION_NUM, 22).T[0:21].T\n",
    "\n",
    "        x = convert_reward_net_input(1, state, action)\n",
    "        reward = self.train_model.reward_network(x)\n",
    "\n",
    "        if rand.random() > eps_threshold:\n",
    "            act_index = int(tf.argmax(reward[0], 0))\n",
    "        else:\n",
    "            act_index = rand.randint(0, ACTION_NUM-1)\n",
    "\n",
    "        return action[act_index], act_index, eps_threshold\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # reward_net_loss = self.reward_learn()\n",
    "\n",
    "        # if reward_net_loss > 10:\n",
    "        #     return\n",
    "\n",
    "        action_net_loss = self.action_learn()\n",
    "\n",
    "        # if action_net_loss > 10:\n",
    "        #     return\n",
    "\n",
    "        self.dqn_learn()\n",
    "\n",
    "    def action_learn(self):\n",
    "        states, actions, action_indexs, rewards, next_states, dones = self.convert_memory(len(self.max_reward_actions))\n",
    "\n",
    "        max_reward = []\n",
    "        for val in self.max_reward_actions.values():\n",
    "            max_reward.append(val[1])\n",
    "        max_reward = tf.convert_to_tensor(max_reward)\n",
    "\n",
    "        expected_action = self.train_model.action_network(states).reshape(len(self.max_reward_actions), ACTION_NUM, 22).T[0:21].T\n",
    "\n",
    "        x = convert_reward_net_input(len(self.max_reward_actions), states, expected_action)\n",
    "        expected_reward = self.train_model.reward_network(x)\n",
    "\n",
    "        a = tf.cast(max_reward.reshape(len(self.max_reward_actions), 1), tf.float32)\n",
    "        b = expected_reward.reshape(len(self.max_reward_actions), 2).T[0].T.reshape(len(self.max_reward_actions), 1)\n",
    "\n",
    "        reward_loss = tf.reduce_sum(tf.math.subtract(a, b))\n",
    "        reward_loss = tf.convert_to_tensor([reward_loss for i in range(len(self.max_reward_actions))]).reshape(len(self.max_reward_actions), 1)\n",
    "        \n",
    "        trick_real = np.concatenate((next_states.reshape(len(self.max_reward_actions), 21), reward_loss), axis=1)\n",
    "        # input_trick = trick_real.flatten()\n",
    "        self.train_model.action_network.fit(states, trick_real, epochs=5, verbose=0)\n",
    "        return self.train_model.action_network.evaluate(states, trick_real, verbose=0)\n",
    "\n",
    "    def reward_learn(self):\n",
    "        states, actions, action_indexs, rewards, next_states, dones = self.convert_memory(BATCH_SIZE)\n",
    "\n",
    "        actions = tf.convert_to_tensor([actions for i in range(ACTION_NUM)]).reshape(BATCH_SIZE, ACTION_NUM, 21)\n",
    "        \n",
    "        x = convert_reward_net_input(BATCH_SIZE, states, actions)\n",
    "        predicted_reward = self.train_model.reward_network(x)\n",
    "        \n",
    "        self.train_model.reward_network.fit(x, rewards, epochs=5, verbose=0)\n",
    "        return self.train_model.reward_network.evaluate(x, rewards, verbose=0)\n",
    "\n",
    "    def dqn_learn(self):\n",
    "        states, actions, action_indexs, rewards, next_states, dones = self.convert_memory(BATCH_SIZE)\n",
    "\n",
    "        current_q = self.train_model(tf.cast(states, tf.float32)).numpy()\n",
    "        next_q = self.target_model(next_states)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                next_q_value = rewards[i]\n",
    "            else:\n",
    "                next_q_value = rewards[i] + GAMMA * np.max(next_q[i])\n",
    "\n",
    "            current_q[i][action_indexs[i]] = next_q_value\n",
    "\n",
    "        states = states.reshape(BATCH_SIZE, 21)\n",
    "\n",
    "        self.train_model.fit(states, current_q, epochs=5, verbose=0)\n",
    "        return self.train_model.evaluate(states, current_q, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 10, reward: [[-0.270316]], pop: [[0.440124]]\n",
      "steps: 20, reward: [[-0.564475]], pop: [[0.394028]]\n",
      "steps: 30, reward: [[-1.363472]], pop: [[0.188757]]\n",
      "steps: 40, reward: [[-3.139391]], pop: [[0.010101]]\n",
      "steps: 50, reward: [[-8.07559]], pop: [[0.]]\n",
      "steps: 60, reward: [[-20.372745]], pop: [[0.876909]]\n",
      "steps: 70, reward: [[-39.157586]], pop: [[0.]]\n",
      "steps: 80, reward: [[-85.800047]], pop: [[0.]]\n",
      "steps: 90, reward: [[-210.10703]], pop: [[0.]]\n",
      "steps: 100, reward: [[-545.325334]], pop: [[0.]]\n",
      "steps: 110, reward: [[-1172.93246]], pop: [[0.]]\n",
      "steps: 120, reward: [[-3405.710475]], pop: [[0.]]\n",
      "(32, 21)\n",
      "(32, 21)\n",
      "(32, 21)\n",
      "(32, 21)\n",
      "(32, 21)\n",
      "steps: 130, reward: [[-6862.450376]], pop: [[0.]]\n",
      "steps: 140, reward: [[-6944.195501]], pop: [[0.]]\n",
      "steps: 150, reward: [[-6944.202616]], pop: [[0.]]\n",
      "steps: 160, reward: [[-6944.180812]], pop: [[0.]]\n",
      "steps: 170, reward: [[-6943.951026]], pop: [[0.]]\n",
      "steps: 180, reward: [[-6943.885778]], pop: [[0.]]\n",
      "steps: 190, reward: [[-6943.871137]], pop: [[0.]]\n",
      "steps: 200, reward: [[-6943.875018]], pop: [[0.]]\n",
      "steps: 210, reward: [[-6943.894066]], pop: [[0.]]\n",
      "steps: 220, reward: [[-6943.913604]], pop: [[0.]]\n",
      "steps: 230, reward: [[-6943.934263]], pop: [[0.]]\n",
      "steps: 240, reward: [[-6943.955384]], pop: [[0.]]\n",
      "steps: 250, reward: [[-6943.975511]], pop: [[0.]]\n",
      "steps: 260, reward: [[-6943.997069]], pop: [[0.]]\n",
      "steps: 270, reward: [[-6944.018636]], pop: [[0.]]\n",
      "steps: 280, reward: [[-6944.040229]], pop: [[0.]]\n",
      "steps: 290, reward: [[-6944.060818]], pop: [[0.]]\n",
      "steps: 300, reward: [[-6944.080708]], pop: [[0.]]\n",
      "steps: 310, reward: [[-6944.101426]], pop: [[0.]]\n",
      "steps: 320, reward: [[-6944.122399]], pop: [[0.]]\n",
      "steps: 330, reward: [[-6944.143094]], pop: [[0.]]\n",
      "steps: 340, reward: [[-6944.164085]], pop: [[0.]]\n",
      "steps: 350, reward: [[-6944.185438]], pop: [[0.]]\n",
      "steps: 360, reward: [[-6944.206398]], pop: [[0.]]\n",
      "steps: 370, reward: [[-6944.227508]], pop: [[0.]]\n",
      "steps: 380, reward: [[-6944.248315]], pop: [[0.]]\n",
      "steps: 390, reward: [[-6944.269129]], pop: [[0.]]\n",
      "steps: 400, reward: [[-6944.289048]], pop: [[0.]]\n",
      "steps: 410, reward: [[-6944.310384]], pop: [[0.]]\n",
      "steps: 420, reward: [[-6944.33144]], pop: [[0.]]\n",
      "steps: 430, reward: [[-6944.352167]], pop: [[0.]]\n",
      "steps: 440, reward: [[-6944.373316]], pop: [[0.]]\n",
      "steps: 450, reward: [[-6944.394362]], pop: [[0.]]\n",
      "steps: 460, reward: [[-6944.415375]], pop: [[0.]]\n",
      "steps: 470, reward: [[-6944.436558]], pop: [[0.]]\n",
      "steps: 480, reward: [[-6944.457674]], pop: [[0.]]\n",
      "steps: 490, reward: [[-6944.478435]], pop: [[0.]]\n",
      "steps: 500, reward: [[-6944.49933]], pop: [[0.]]\n",
      "steps: 510, reward: [[-6944.52085]], pop: [[0.]]\n",
      "steps: 520, reward: [[-6944.542133]], pop: [[0.]]\n",
      "steps: 530, reward: [[-6944.562916]], pop: [[0.]]\n",
      "steps: 540, reward: [[-6944.583589]], pop: [[0.]]\n",
      "steps: 550, reward: [[-6944.604169]], pop: [[0.]]\n",
      "steps: 560, reward: [[-6944.625333]], pop: [[0.]]\n",
      "steps: 570, reward: [[-6944.646352]], pop: [[0.]]\n",
      "steps: 580, reward: [[-6944.667111]], pop: [[0.]]\n",
      "steps: 590, reward: [[-6944.688186]], pop: [[0.]]\n",
      "steps: 600, reward: [[-6944.709043]], pop: [[0.]]\n",
      "steps: 610, reward: [[-6944.729537]], pop: [[0.]]\n",
      "steps: 620, reward: [[-6944.750785]], pop: [[0.]]\n",
      "steps: 630, reward: [[-6944.771849]], pop: [[0.]]\n",
      "steps: 640, reward: [[-6944.793355]], pop: [[0.]]\n",
      "steps: 650, reward: [[-6944.814349]], pop: [[0.]]\n",
      "steps: 660, reward: [[-6944.834003]], pop: [[0.]]\n",
      "steps: 670, reward: [[-6944.855084]], pop: [[0.]]\n",
      "steps: 680, reward: [[-6944.876358]], pop: [[0.]]\n",
      "steps: 690, reward: [[-6944.897393]], pop: [[0.]]\n",
      "steps: 700, reward: [[-6944.918479]], pop: [[0.]]\n",
      "steps: 710, reward: [[-6944.939392]], pop: [[0.]]\n",
      "steps: 720, reward: [[-6944.960398]], pop: [[0.]]\n",
      "steps: 730, reward: [[-6944.981574]], pop: [[0.]]\n",
      "steps: 740, reward: [[-6945.002445]], pop: [[0.]]\n",
      "steps: 750, reward: [[-6945.023716]], pop: [[0.]]\n",
      "steps: 760, reward: [[-6945.044261]], pop: [[0.]]\n",
      "steps: 770, reward: [[-6945.065495]], pop: [[0.]]\n",
      "steps: 780, reward: [[-6945.08664]], pop: [[0.]]\n",
      "steps: 790, reward: [[-6945.107848]], pop: [[0.]]\n",
      "steps: 800, reward: [[-6945.12897]], pop: [[0.]]\n",
      "steps: 810, reward: [[-6945.149946]], pop: [[0.]]\n",
      "steps: 820, reward: [[-6945.17098]], pop: [[0.]]\n",
      "steps: 830, reward: [[-6945.1917]], pop: [[0.]]\n",
      "steps: 840, reward: [[-6945.212519]], pop: [[0.]]\n",
      "steps: 850, reward: [[-6945.232942]], pop: [[0.]]\n",
      "steps: 860, reward: [[-6945.253897]], pop: [[0.]]\n",
      "steps: 870, reward: [[-6945.275355]], pop: [[0.]]\n",
      "steps: 880, reward: [[-6945.29658]], pop: [[0.]]\n",
      "steps: 890, reward: [[-6945.317721]], pop: [[0.]]\n",
      "steps: 900, reward: [[-6945.338996]], pop: [[0.]]\n",
      "steps: 910, reward: [[-6945.359667]], pop: [[0.]]\n",
      "steps: 920, reward: [[-6945.381295]], pop: [[0.]]\n",
      "steps: 930, reward: [[-6945.40238]], pop: [[0.]]\n",
      "steps: 940, reward: [[-6945.42353]], pop: [[0.]]\n",
      "steps: 950, reward: [[-6945.444446]], pop: [[0.]]\n",
      "steps: 960, reward: [[-6945.465576]], pop: [[0.]]\n",
      "steps: 970, reward: [[-6945.486377]], pop: [[0.]]\n",
      "steps: 980, reward: [[-6945.507543]], pop: [[0.]]\n",
      "steps: 990, reward: [[-6945.528938]], pop: [[0.]]\n",
      "steps: 1000, reward: [[-6945.550182]], pop: [[0.]]\n",
      "==================0 done==================\n",
      "[[-6945.55229]] 0.9983366666666667\n",
      "==============================================\n",
      "steps: 10, reward: [[-0.094063]], pop: [[0.500282]]\n",
      "steps: 20, reward: [[-0.205569]], pop: [[0.489296]]\n",
      "steps: 30, reward: [[-0.324289]], pop: [[0.471168]]\n",
      "steps: 40, reward: [[-0.448]], pop: [[0.447956]]\n",
      "steps: 50, reward: [[-0.570447]], pop: [[0.426185]]\n",
      "steps: 60, reward: [[-0.69416]], pop: [[0.402966]]\n",
      "steps: 70, reward: [[-0.815421]], pop: [[0.38185]]\n",
      "steps: 80, reward: [[-0.937092]], pop: [[0.361251]]\n",
      "steps: 90, reward: [[-1.058904]], pop: [[0.341725]]\n",
      "steps: 100, reward: [[-1.178334]], pop: [[0.322818]]\n",
      "steps: 110, reward: [[-1.29763]], pop: [[0.304651]]\n",
      "steps: 120, reward: [[-1.416935]], pop: [[0.286467]]\n",
      "steps: 130, reward: [[-1.53485]], pop: [[0.269768]]\n",
      "steps: 140, reward: [[-1.650923]], pop: [[0.254448]]\n",
      "steps: 150, reward: [[-1.766353]], pop: [[0.239684]]\n",
      "steps: 160, reward: [[-1.881342]], pop: [[0.225565]]\n",
      "steps: 170, reward: [[-1.995244]], pop: [[0.212068]]\n",
      "steps: 180, reward: [[-2.110544]], pop: [[0.198603]]\n",
      "steps: 190, reward: [[-2.223613]], pop: [[0.186115]]\n",
      "steps: 200, reward: [[-2.335964]], pop: [[0.174361]]\n",
      "steps: 210, reward: [[-2.448214]], pop: [[0.16309]]\n",
      "steps: 220, reward: [[-2.559]], pop: [[0.152633]]\n",
      "steps: 230, reward: [[-2.669853]], pop: [[0.14258]]\n",
      "steps: 240, reward: [[-2.779728]], pop: [[0.133226]]\n",
      "steps: 250, reward: [[-2.889125]], pop: [[0.124356]]\n",
      "steps: 260, reward: [[-2.997816]], pop: [[0.116052]]\n",
      "steps: 270, reward: [[-3.105674]], pop: [[0.108349]]\n",
      "steps: 280, reward: [[-3.213732]], pop: [[0.100923]]\n",
      "steps: 290, reward: [[-3.321369]], pop: [[0.093992]]\n",
      "steps: 300, reward: [[-3.429325]], pop: [[0.087242]]\n",
      "steps: 310, reward: [[-3.535844]], pop: [[0.081213]]\n",
      "steps: 320, reward: [[-3.642006]], pop: [[0.075555]]\n",
      "steps: 330, reward: [[-3.7479]], pop: [[0.070224]]\n",
      "steps: 340, reward: [[-3.853111]], pop: [[0.065301]]\n",
      "steps: 350, reward: [[-3.958723]], pop: [[0.060548]]\n",
      "steps: 360, reward: [[-4.063608]], pop: [[0.056234]]\n",
      "steps: 370, reward: [[-4.167998]], pop: [[0.052233]]\n",
      "steps: 380, reward: [[-4.272358]], pop: [[0.048441]]\n",
      "steps: 390, reward: [[-4.376307]], pop: [[0.044946]]\n",
      "steps: 400, reward: [[-4.480361]], pop: [[0.041645]]\n",
      "steps: 410, reward: [[-4.58402]], pop: [[0.038604]]\n",
      "steps: 420, reward: [[-4.68733]], pop: [[0.035792]]\n",
      "steps: 430, reward: [[-4.790611]], pop: [[0.033174]]\n",
      "steps: 440, reward: [[-4.893636]], pop: [[0.030731]]\n",
      "steps: 450, reward: [[-4.996582]], pop: [[0.028447]]\n",
      "steps: 460, reward: [[-5.099358]], pop: [[0.026298]]\n",
      "steps: 470, reward: [[-5.201689]], pop: [[0.024346]]\n",
      "steps: 480, reward: [[-5.304241]], pop: [[0.022517]]\n",
      "steps: 490, reward: [[-5.406445]], pop: [[0.020835]]\n",
      "steps: 500, reward: [[-5.508596]], pop: [[0.019281]]\n",
      "steps: 510, reward: [[-5.610768]], pop: [[0.017825]]\n",
      "steps: 520, reward: [[-5.712713]], pop: [[0.016484]]\n",
      "steps: 530, reward: [[-5.814708]], pop: [[0.01523]]\n",
      "steps: 540, reward: [[-5.916717]], pop: [[0.014062]]\n",
      "steps: 550, reward: [[-6.018173]], pop: [[0.013012]]\n",
      "steps: 560, reward: [[-6.119776]], pop: [[0.012024]]\n",
      "steps: 570, reward: [[-6.221126]], pop: [[0.011132]]\n",
      "steps: 580, reward: [[-6.32266]], pop: [[0.010285]]\n",
      "steps: 590, reward: [[-6.423749]], pop: [[0.009515]]\n",
      "steps: 600, reward: [[-6.524482]], pop: [[0.008827]]\n",
      "steps: 610, reward: [[-6.625795]], pop: [[0.008157]]\n",
      "steps: 620, reward: [[-6.727434]], pop: [[0.007523]]\n",
      "steps: 630, reward: [[-6.828522]], pop: [[0.006962]]\n",
      "steps: 640, reward: [[-6.929771]], pop: [[0.006433]]\n",
      "steps: 650, reward: [[-7.030835]], pop: [[0.005948]]\n",
      "steps: 660, reward: [[-7.131747]], pop: [[0.005513]]\n",
      "steps: 670, reward: [[-7.232478]], pop: [[0.005113]]\n",
      "steps: 680, reward: [[-7.333625]], pop: [[0.004716]]\n",
      "steps: 690, reward: [[-7.43483]], pop: [[0.004349]]\n",
      "steps: 700, reward: [[-7.535805]], pop: [[0.004014]]\n",
      "steps: 710, reward: [[-7.636681]], pop: [[0.003715]]\n",
      "steps: 720, reward: [[-7.737527]], pop: [[0.003435]]\n",
      "steps: 730, reward: [[-7.838382]], pop: [[0.003172]]\n",
      "steps: 740, reward: [[-7.939377]], pop: [[0.002924]]\n",
      "steps: 750, reward: [[-8.040068]], pop: [[0.002696]]\n",
      "steps: 760, reward: [[-8.140876]], pop: [[0.002491]]\n",
      "steps: 770, reward: [[-8.241714]], pop: [[0.002303]]\n",
      "steps: 780, reward: [[-8.342464]], pop: [[0.002133]]\n",
      "steps: 790, reward: [[-8.443452]], pop: [[0.001968]]\n",
      "steps: 800, reward: [[-8.544387]], pop: [[0.001816]]\n",
      "steps: 810, reward: [[-8.645111]], pop: [[0.001679]]\n",
      "steps: 820, reward: [[-8.746056]], pop: [[0.001549]]\n",
      "steps: 830, reward: [[-8.846726]], pop: [[0.001433]]\n",
      "steps: 840, reward: [[-8.947475]], pop: [[0.001324]]\n",
      "steps: 850, reward: [[-9.048372]], pop: [[0.001222]]\n",
      "steps: 860, reward: [[-9.148933]], pop: [[0.001136]]\n",
      "steps: 870, reward: [[-9.249857]], pop: [[0.001048]]\n",
      "steps: 880, reward: [[-9.35065]], pop: [[0.000968]]\n",
      "steps: 890, reward: [[-9.451496]], pop: [[0.000894]]\n",
      "steps: 900, reward: [[-9.552235]], pop: [[0.000827]]\n",
      "steps: 910, reward: [[-9.652911]], pop: [[0.000764]]\n",
      "steps: 920, reward: [[-9.753599]], pop: [[0.000703]]\n",
      "steps: 930, reward: [[-9.854083]], pop: [[0.000649]]\n",
      "steps: 940, reward: [[-9.954444]], pop: [[0.0006]]\n",
      "steps: 950, reward: [[-10.05498]], pop: [[0.000554]]\n",
      "steps: 960, reward: [[-10.155422]], pop: [[0.000514]]\n",
      "steps: 970, reward: [[-10.255837]], pop: [[0.000477]]\n",
      "steps: 980, reward: [[-10.356755]], pop: [[0.000439]]\n",
      "steps: 990, reward: [[-10.457263]], pop: [[0.000407]]\n",
      "steps: 1000, reward: [[-10.558139]], pop: [[0.000377]]\n",
      "==================1 done==================\n",
      "[[-10.568205]] 0.99668\n",
      "==============================================\n",
      "steps: 10, reward: [[-0.09405]], pop: [[0.500474]]\n",
      "steps: 20, reward: [[-0.206213]], pop: [[0.488955]]\n",
      "steps: 30, reward: [[-0.323489]], pop: [[0.47229]]\n",
      "steps: 40, reward: [[-0.44864]], pop: [[0.447995]]\n",
      "steps: 50, reward: [[-0.57204]], pop: [[0.425367]]\n",
      "steps: 60, reward: [[-0.695957]], pop: [[0.402173]]\n",
      "steps: 70, reward: [[-0.81981]], pop: [[0.379326]]\n",
      "steps: 80, reward: [[-0.940854]], pop: [[0.358879]]\n",
      "steps: 90, reward: [[-1.062396]], pop: [[0.338201]]\n",
      "steps: 100, reward: [[-1.180879]], pop: [[0.320287]]\n",
      "steps: 110, reward: [[-1.300137]], pop: [[0.301823]]\n",
      "steps: 120, reward: [[-1.418424]], pop: [[0.284122]]\n",
      "steps: 130, reward: [[-1.535048]], pop: [[0.268062]]\n",
      "steps: 140, reward: [[-1.650897]], pop: [[0.252834]]\n",
      "steps: 150, reward: [[-1.7669]], pop: [[0.237619]]\n",
      "steps: 160, reward: [[-1.881126]], pop: [[0.223938]]\n",
      "steps: 170, reward: [[-1.995538]], pop: [[0.210143]]\n",
      "steps: 180, reward: [[-2.109601]], pop: [[0.196952]]\n",
      "steps: 190, reward: [[-2.222354]], pop: [[0.184851]]\n",
      "steps: 200, reward: [[-2.335178]], pop: [[0.172828]]\n",
      "steps: 210, reward: [[-2.447153]], pop: [[0.161702]]\n",
      "steps: 220, reward: [[-2.558552]], pop: [[0.150944]]\n",
      "steps: 230, reward: [[-2.66995]], pop: [[0.140547]]\n",
      "steps: 240, reward: [[-2.780071]], pop: [[0.131126]]\n",
      "steps: 250, reward: [[-2.889358]], pop: [[0.122499]]\n",
      "steps: 260, reward: [[-2.997647]], pop: [[0.114647]]\n",
      "steps: 270, reward: [[-3.106695]], pop: [[0.106516]]\n",
      "steps: 280, reward: [[-3.214815]], pop: [[0.099162]]\n",
      "steps: 290, reward: [[-3.321978]], pop: [[0.092481]]\n",
      "steps: 300, reward: [[-3.428749]], pop: [[0.086234]]\n",
      "steps: 310, reward: [[-3.534968]], pop: [[0.080467]]\n",
      "steps: 320, reward: [[-3.641053]], pop: [[0.07509]]\n",
      "steps: 330, reward: [[-3.747427]], pop: [[0.069482]]\n",
      "steps: 340, reward: [[-3.853105]], pop: [[0.064517]]\n",
      "steps: 350, reward: [[-3.958393]], pop: [[0.060015]]\n",
      "steps: 360, reward: [[-4.063434]], pop: [[0.055513]]\n",
      "steps: 370, reward: [[-4.168487]], pop: [[0.051291]]\n",
      "steps: 380, reward: [[-4.272986]], pop: [[0.047685]]\n",
      "steps: 390, reward: [[-4.376705]], pop: [[0.044199]]\n",
      "steps: 400, reward: [[-4.480305]], pop: [[0.041234]]\n",
      "steps: 410, reward: [[-4.584121]], pop: [[0.0381]]\n",
      "steps: 420, reward: [[-4.687734]], pop: [[0.035203]]\n",
      "steps: 430, reward: [[-4.791415]], pop: [[0.032366]]\n",
      "steps: 440, reward: [[-4.894425]], pop: [[0.029888]]\n",
      "steps: 450, reward: [[-4.997823]], pop: [[0.027504]]\n",
      "steps: 460, reward: [[-5.100484]], pop: [[0.025493]]\n",
      "steps: 470, reward: [[-5.202793]], pop: [[0.023657]]\n",
      "steps: 480, reward: [[-5.305151]], pop: [[0.021975]]\n",
      "steps: 490, reward: [[-5.407321]], pop: [[0.02033]]\n",
      "steps: 500, reward: [[-5.509323]], pop: [[0.018716]]\n",
      "steps: 510, reward: [[-5.611664]], pop: [[0.017401]]\n",
      "steps: 520, reward: [[-5.713774]], pop: [[0.01602]]\n",
      "steps: 530, reward: [[-5.815158]], pop: [[0.014817]]\n",
      "steps: 540, reward: [[-5.916769]], pop: [[0.013636]]\n",
      "steps: 550, reward: [[-6.01832]], pop: [[0.012581]]\n",
      "steps: 560, reward: [[-6.119933]], pop: [[0.011672]]\n",
      "steps: 570, reward: [[-6.220806]], pop: [[0.010725]]\n",
      "steps: 580, reward: [[-6.322073]], pop: [[0.009887]]\n",
      "steps: 590, reward: [[-6.422848]], pop: [[0.00914]]\n",
      "steps: 600, reward: [[-6.523958]], pop: [[0.008423]]\n",
      "steps: 610, reward: [[-6.625048]], pop: [[0.0078]]\n",
      "steps: 620, reward: [[-6.72606]], pop: [[0.007183]]\n",
      "steps: 630, reward: [[-6.827163]], pop: [[0.006644]]\n",
      "steps: 640, reward: [[-6.928127]], pop: [[0.006149]]\n",
      "steps: 650, reward: [[-7.029666]], pop: [[0.00568]]\n",
      "steps: 660, reward: [[-7.130686]], pop: [[0.005248]]\n",
      "steps: 670, reward: [[-7.231517]], pop: [[0.004841]]\n",
      "steps: 680, reward: [[-7.332233]], pop: [[0.004483]]\n",
      "steps: 690, reward: [[-7.432837]], pop: [[0.004149]]\n",
      "steps: 700, reward: [[-7.533705]], pop: [[0.003829]]\n",
      "steps: 710, reward: [[-7.634226]], pop: [[0.003547]]\n",
      "steps: 720, reward: [[-7.734921]], pop: [[0.003275]]\n",
      "steps: 730, reward: [[-7.835679]], pop: [[0.003027]]\n",
      "steps: 740, reward: [[-7.936917]], pop: [[0.002791]]\n",
      "steps: 750, reward: [[-8.037494]], pop: [[0.002579]]\n",
      "steps: 760, reward: [[-8.137805]], pop: [[0.002379]]\n",
      "steps: 770, reward: [[-8.238666]], pop: [[0.002201]]\n",
      "steps: 780, reward: [[-8.339342]], pop: [[0.002035]]\n",
      "steps: 790, reward: [[-8.440101]], pop: [[0.001883]]\n",
      "steps: 800, reward: [[-8.540598]], pop: [[0.001735]]\n",
      "steps: 810, reward: [[-8.64128]], pop: [[0.001603]]\n",
      "steps: 820, reward: [[-8.741639]], pop: [[0.001482]]\n",
      "steps: 830, reward: [[-8.84234]], pop: [[0.001368]]\n",
      "steps: 840, reward: [[-8.943228]], pop: [[0.001267]]\n",
      "steps: 850, reward: [[-9.044162]], pop: [[0.001173]]\n",
      "steps: 860, reward: [[-9.144796]], pop: [[0.001082]]\n",
      "steps: 870, reward: [[-9.245466]], pop: [[0.001]]\n",
      "steps: 880, reward: [[-9.345969]], pop: [[0.000924]]\n",
      "steps: 890, reward: [[-9.446452]], pop: [[0.000854]]\n",
      "steps: 900, reward: [[-9.547002]], pop: [[0.000788]]\n",
      "steps: 910, reward: [[-9.647234]], pop: [[0.000729]]\n",
      "steps: 920, reward: [[-9.747984]], pop: [[0.000674]]\n",
      "steps: 930, reward: [[-9.848734]], pop: [[0.000621]]\n",
      "steps: 940, reward: [[-9.949256]], pop: [[0.000575]]\n",
      "steps: 950, reward: [[-10.049813]], pop: [[0.000531]]\n",
      "steps: 960, reward: [[-10.149995]], pop: [[0.000492]]\n",
      "steps: 970, reward: [[-10.250646]], pop: [[0.000454]]\n",
      "steps: 980, reward: [[-10.351444]], pop: [[0.000421]]\n",
      "steps: 990, reward: [[-10.452239]], pop: [[0.000388]]\n",
      "steps: 1000, reward: [[-10.552791]], pop: [[0.000359]]\n",
      "==================2 done==================\n",
      "[[-10.562868]] 0.99503\n",
      "==============================================\n",
      "steps: 10, reward: [[-0.093744]], pop: [[0.500353]]\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_Agent()\n",
    "sc_hist = []\n",
    "st_hist = []\n",
    "pop_hist = []\n",
    "\n",
    "for e in range(1000):\n",
    "    state = DATUM_STATE\n",
    "    steps = 0\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        agent.target_model.set_weights(agent.target_model.get_weights())\n",
    "    \n",
    "    while True:\n",
    "        pred_y = dnn_model.predict(state, verbose=0)\n",
    "\n",
    "        action, idx, eps = agent.act(state)\n",
    "\n",
    "        next_state = return_state(state, action)\n",
    "        reward = return_reward(state, pred_y)\n",
    "\n",
    "        if steps == EPISODE_DONE or abs(reward[0][0]) > 1000000:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        agent.memorize(state, action, idx, reward, next_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 10 == 0:\n",
    "            print(f\"steps: {steps}, reward: {reward}, pop: {pred_y}\")\n",
    "\n",
    "        if done:\n",
    "            print(f\"=================={e} done==================\")\n",
    "            print(reward, eps)\n",
    "            print(\"==============================================\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
