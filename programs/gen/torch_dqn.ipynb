{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turtle():\n",
    "    print(\"\"\"\t\t      거북거북거북거북거북거북거북\n",
    "\t\t    거북거북거북거북거북거북거북거북\n",
    "\t\t  거북거북거북거북거북거북거북거북거북\n",
    "\t\t거북거북거북거북거북거북거북거북거북거북\n",
    "  북거북거북      거북거북거북거북거북거북거북거북거북거북거북\n",
    "거북거북거북거북거북거북거북거북거북거북거북거북거북거북거북거북거북이 꼬북이\n",
    "  북거북거북      거북거북거북거북거북거북거북거북거북거북거북\n",
    "\t            거북거북\t 거\t \t   거\t거북거북\n",
    " \t\t거북거\t북\t   \t    북\t북거북\n",
    "\t\t 거북거\t거\t   \t   거\t  북거\n",
    "\t\t 거북\t\t\t\t거북\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t      거북거북거북거북거북거북거북\n",
      "\t\t    거북거북거북거북거북거북거북거북\n",
      "\t\t  거북거북거북거북거북거북거북거북거북\n",
      "\t\t거북거북거북거북거북거북거북거북거북거북\n",
      "  북거북거북      거북거북거북거북거북거북거북거북거북거북거북\n",
      "거북거북거북거북거북거북거북거북거북거북거북거북거북거북거북거북거북이 꼬북이\n",
      "  북거북거북      거북거북거북거북거북거북거북거북거북거북거북\n",
      "\t            거북거북\t 거\t \t   거\t거북거북\n",
      " \t\t거북거\t북\t   \t    북\t북거북\n",
      "\t\t 거북거\t거\t   \t   거\t  북거\n",
      "\t\t 거북\t\t\t\t거북\n",
      "\n"
     ]
    }
   ],
   "source": [
    "turtle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('c:\\\\code\\\\activ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = 'nov_nine_var.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = tf.keras.models.load_model('./model/dnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn 에이전트용 하이퍼 파라미터 정의\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 100\n",
    "\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 2\n",
    "ACTION_NUM = 2\n",
    "EPISODE_DONE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./documents/'+df_name).iloc[:,1:23]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df.iloc[:,0:21])\n",
    "DATUM_STATE = X[-1].reshape(1, 21) # 거리를 계산하기 위해 기준점을 설정\n",
    "DATUM_RATE = df.iloc[:,21:22].iloc[-1].to_numpy() # 제일 최근 데이터의 수도권 과밀화율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_scaler = MinMaxScaler()\n",
    "y = pop_scaler.fit_transform(df.iloc[:,21:22].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_rate_represent_df = pd.read_excel('./documents/other/change_rate_representative.xlsx').iloc[:,1::]\n",
    "FLOOR = change_rate_represent_df[0]\n",
    "TOP = change_rate_represent_df[1]\n",
    "MODE = change_rate_represent_df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORR = df.corr().iloc[-1].to_numpy()[0:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(s, a):\n",
    "    # 현 상황에서 한 행동에 대한 상황\n",
    "    # 즉, 다음 상황을 반환\n",
    "    return a.detach().numpy()+s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(next_state, y_pred):\n",
    "    # dqn 에이전트가 학습할 보상\n",
    "    dist = np.sqrt(np.sum(np.square(DATUM_STATE-next_state)))\n",
    "    pop = DATUM_RATE - y_pred\n",
    "\n",
    "    return -10*(dist+pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_loss(act):\n",
    "    # 보폭과 변수 상관관계 계산\n",
    "    # 즉, 행동에 대한 손실\n",
    "    loss = 0\n",
    "    actions = act.detach().numpy()\n",
    "\n",
    "    # BATCH_SIZE, ACTION_NUM, 21\n",
    "    for batch_action in actions:\n",
    "        for act in batch_action:\n",
    "            for i, var_act in enumerate(act):\n",
    "                if var_act < FLOOR[i] or var_act > TOP[i]:\n",
    "                    loss += 0.1\n",
    "\n",
    "                loss += abs(var_act - MODE[i])\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_loss(s, a):\n",
    "    # ns * x = pp\n",
    "    # ns/pp = 1/x\n",
    "    # x = pp/ns\n",
    "    # corr_loss = (corr-x)^2\n",
    "    ns = return_state(s.numpy(), a)\n",
    "    pp = dnn_model(ns)\n",
    "\n",
    "    pp = pop_scaler.transform(pp)\n",
    "    \n",
    "    corr_loss = 0\n",
    "    for i, var_ns in enumerate(ns):\n",
    "        corr_loss += (CORR[i]-(pp/var_ns)) ** 2\n",
    "    return corr_loss/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action_network(torch.nn.Module):\n",
    "    # 현재 상황을 입력 받아서 행동을 출력하는 신경망\n",
    "    # 보폭, 변수 상관관계, 보상 고려\n",
    "    def __init__(self):\n",
    "        super(Action_network, self).__init__()\n",
    "        self.input_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=21, out_features=64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.hidden_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 21*ACTION_NUM)\n",
    "        )\n",
    "        \n",
    "    def forward(self, s):\n",
    "        i = self.input_layer(s)\n",
    "        h = self.hidden_layer(i)\n",
    "        o = self.output_layer(h)\n",
    "        o = o.reshape(len(s), ACTION_NUM, 21)\n",
    "\n",
    "        # print(f\"action network: state: {s.shape}, out: {o.shape}\")\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward_network(torch.nn.Module):\n",
    "    # 현재 상황과 행동을 입력 받아서 보상을 출력하는 신경망\n",
    "    def __init__(self):\n",
    "        super(Reward_network, self).__init__()\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.input_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=ACTION_NUM*2*21, out_features=64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.hidden_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 64),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = torch.nn.Sequential(\n",
    "            torch.nn.Linear(64, ACTION_NUM)\n",
    "        )\n",
    "\n",
    "    def forward(self, data_num, s, a):\n",
    "        s = torch.stack([s for i in range(ACTION_NUM)]).reshape(data_num, ACTION_NUM, 21)\n",
    "        a = a.reshape(data_num, ACTION_NUM, 21)\n",
    "        x = torch.stack([s, a]).reshape(data_num, ACTION_NUM, 2, 21)\n",
    "\n",
    "        f = self.flat(x)\n",
    "        i = self.input_layer(f)\n",
    "        h = self.hidden_layer(i)\n",
    "        o = self.output_layer(h)\n",
    "\n",
    "        # print(f\"reward network: state: {s.shape}, action: {a.shape}, out: {o.shape}\")\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn_network(torch.nn.Module):\n",
    "    # 상황을 입력 받아 보상을 출력하는 신경망\n",
    "    def __init__(self):\n",
    "        super(Dqn_network, self).__init__()\n",
    "        self.action_network = Action_network()\n",
    "        self.reward_network = Reward_network()\n",
    "    \n",
    "    def forward(self, s):\n",
    "        action = self.action_network(s)\n",
    "        reward = self.reward_network(BATCH_SIZE, s, action)\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn_agent:\n",
    "    def __init__(self):\n",
    "        self.train_model = Dqn_network()\n",
    "        self.target_model = Dqn_network()\n",
    "\n",
    "        self.dqn_optimizer = torch.optim.Adam(self.train_model.parameters(), 0.00025)\n",
    "        self.action_optimizer = torch.optim.Adam(self.train_model.action_network.parameters(), 0.00025)\n",
    "        self.reward_optimizer = torch.optim.Adam(self.train_model.reward_network.parameters(), 0.00025)\n",
    "\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        self.max_reward_actions = {}\n",
    "        \n",
    "        self.episode = 1\n",
    "\n",
    "    def memorize(self, state, action, act_idx, reward, next_state, step):\n",
    "        if step == EPISODE_DONE:\n",
    "            done = True\n",
    "            self.episode += 1\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.memory.append(\n",
    "            (\n",
    "                torch.from_numpy(state).to(torch.float32),\n",
    "                action,\n",
    "                act_idx,\n",
    "                torch.from_numpy(reward).to(torch.float32),\n",
    "                torch.from_numpy(next_state).to(torch.float32),\n",
    "                done\n",
    "            )\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            if self.max_reward_actions[tuple(map(tuple, state))][1] < reward:\n",
    "                self.max_reward_actions[tuple(map(tuple, state))] = [action, reward]\n",
    "\n",
    "        except KeyError:\n",
    "            self.max_reward_actions[tuple(map(tuple, state))] = [action, reward]\n",
    "\n",
    "    def convert_memory(self, length):\n",
    "        batch = rand.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, act_idxs, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(list(states), dim=0).reshape(BATCH_SIZE, 21)\n",
    "        actions = torch.stack(list(actions), dim=0)\n",
    "        act_idxs = torch.tensor(list(act_idxs))\n",
    "        rewards = torch.stack(list(rewards), dim=0)\n",
    "        next_states = torch.stack(list(next_states), dim=0).reshape(BATCH_SIZE, 21)\n",
    "        dones = list(dones)\n",
    "\n",
    "        return states, actions, act_idxs, rewards, next_states, dones\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.episode >= 0 and self.episode < 200:\n",
    "            eps_threshold = -(self.episode/1000)+1+(self.episode)*(self.episode-200)/300000\n",
    "        else:\n",
    "            eps_threshold = -(self.episode/1000)+1+(self.episode-200)*(self.episode-1000)\n",
    "            \n",
    "        state = torch.from_numpy(state).to(torch.float32)\n",
    "        action = self.train_model.action_network(state)\n",
    "        reward = self.train_model.reward_network(1, state, action)\n",
    "\n",
    "        if rand.random() > eps_threshold:\n",
    "            act_index = reward.detach().argmax()\n",
    "        else:\n",
    "            act_index = rand.randint(0, ACTION_NUM-1)\n",
    "\n",
    "        return action[0][act_index], act_index, eps_threshold\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        reward_net_loss = self.reward_learn()\n",
    "\n",
    "        # if reward_net_loss > 10:\n",
    "        #     return\n",
    "        \n",
    "        action_net_loss = self.action_learn()\n",
    "\n",
    "        # if action_net_loss > 10:\n",
    "        #     return\n",
    "        \n",
    "        self.dqn_learn()\n",
    "\n",
    "    def action_learn(self):\n",
    "        states, actions, act_idxs, rewards, next_states, dones = self.convert_memory(BATCH_SIZE)\n",
    "\n",
    "        expected_action = self.train_model.action_network.forward(states)\n",
    "\n",
    "        max_reward = []\n",
    "        for val in self.max_reward_actions.values():\n",
    "            max_reward.append(val[1])\n",
    "        max_reward = torch.tensor(np.array(max_reward))\n",
    "\n",
    "        expected_action = expected_action.reshape(BATCH_SIZE, ACTION_NUM, 21)\n",
    "        expected_reward = self.train_model.reward_network(BATCH_SIZE, states, expected_action)\n",
    "\n",
    "        step_loss = []\n",
    "        for expect in expected_action:\n",
    "            step_loss.append(action_loss(expected_action))\n",
    "        step_loss = torch.tensor(np.array(step_loss))\n",
    "\n",
    "        cor_loss = correlation_loss(states, actions)\n",
    "\n",
    "        loss = max_reward.mean() - expected_reward.mean() + step_loss.mean() + cor_loss.mean()\n",
    "\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.action_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.action_optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def reward_learn(self):\n",
    "        states, actions, act_idxs, rewards, next_states, dones = self.convert_memory(BATCH_SIZE)\n",
    "        actions = torch.stack([actions for i in range(BATCH_SIZE)]).reshape(BATCH_SIZE, ACTION_NUM, 21)\n",
    "        predicted_reward = self.train_model.reward_network.forward(BATCH_SIZE, states, actions)\n",
    "        \n",
    "        expected_reward = []\n",
    "        for i in range(BATCH_SIZE):\n",
    "            sum_predicted_reward = 0\n",
    "            for j in range(ACTION_NUM):\n",
    "                sum_predicted_reward += predicted_reward[i][j]\n",
    "            expected_reward.append(sum_predicted_reward/ACTION_NUM)\n",
    "        expected_reward = torch.stack(expected_reward).reshape(BATCH_SIZE, 1)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(expected_reward, rewards)\n",
    "        self.reward_optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.reward_optimizer.step()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def dqn_learn(self):\n",
    "        states, actions, act_idxs, rewards, next_states, dones = self.convert_memory(BATCH_SIZE)\n",
    "\n",
    "        current_q = self.train_model.forward(states)[0].gather(0, act_idxs).reshape(BATCH_SIZE)\n",
    "        max_next_q = self.target_model.forward(next_states).detach().max(1)[0]\n",
    "\n",
    "        expected_q = torch.empty_like(rewards)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                expected_q[i] = max_next_q\n",
    "            else:\n",
    "                expected_q[i] = rewards[i] + (GAMMA * max_next_q[i])\n",
    "\n",
    "        expected_q = expected_q.reshape(BATCH_SIZE)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(current_q, expected_q)\n",
    "        self.dqn_optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.dqn_optimizer.step()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: 100, reward: [-344.282044], pop: [0.551683]\n",
      "steps: 200, reward: [-1127.45043], pop: [0.775053]\n",
      "steps: 300, reward: [-2784.923052], pop: [0.976532]\n",
      "steps: 400, reward: [-6013.795366], pop: [0.996343]\n",
      "steps: 500, reward: [-12597.170541], pop: [1.]\n",
      "steps: 600, reward: [-26984.28549], pop: [1.]\n",
      "steps: 700, reward: [-61963.143435], pop: [1.]\n",
      "steps: 800, reward: [-131112.408201], pop: [1.]\n",
      "steps: 900, reward: [-254593.380977], pop: [1.]\n",
      "steps: 1000, reward: [-474200.510408], pop: [1.]\n",
      "=============episode done=============\n",
      "==========0, 0.998337==========\n",
      "steps: 100, reward: [-346.424322], pop: [0.515371]\n",
      "steps: 200, reward: [-1134.377694], pop: [0.763639]\n",
      "steps: 300, reward: [-2802.734944], pop: [0.975899]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain_model\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m \u001b[43mdnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     16\u001b[0m     action, action_index, eps \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m     17\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m return_state(state, action)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\keras\\engine\\training.py:2002\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1995\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   1996\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1997\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1998\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mTPUStrategy and AutoShardPolicy.FILE might lead to out-of-order \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1999\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mresult. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   2000\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m-> 2002\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2003\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2004\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2005\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2006\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2007\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2008\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2009\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2010\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2011\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2012\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   2014\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\keras\\engine\\data_adapter.py:1401\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1400\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1401\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\keras\\engine\\data_adapter.py:1151\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1148\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1150\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1152\u001b[0m     x,\n\u001b[0;32m   1153\u001b[0m     y,\n\u001b[0;32m   1154\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1155\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1156\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1157\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1158\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1159\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1160\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1161\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1162\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1163\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1165\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\keras\\engine\\data_adapter.py:326\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m     flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    324\u001b[0m   \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 326\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[0;32m    328\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    330\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2092\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2058\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_map\u001b[39m(\u001b[39mself\u001b[39m, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2059\u001b[0m   \u001b[39m\"\"\"Maps `map_func` across this dataset and flattens the result.\u001b[39;00m\n\u001b[0;32m   2060\u001b[0m \n\u001b[0;32m   2061\u001b[0m \u001b[39m  The type signature is:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[39m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m   2091\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2092\u001b[0m   \u001b[39mreturn\u001b[39;00m FlatMapDataset(\u001b[39mself\u001b[39;49m, map_func, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5327\u001b[0m, in \u001b[0;36mFlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m   5325\u001b[0m \u001b[39m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m   5326\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m-> 5327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m   5328\u001b[0m     map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[0;32m   5329\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, DatasetSpec):\n\u001b[0;32m   5330\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   5331\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   5332\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m_get_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:271\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    265\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    269\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 271\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    272\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2567\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2558\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   2559\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   2560\u001b[0m \n\u001b[0;32m   2561\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2565\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   2566\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2567\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\n\u001b[0;32m   2568\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2569\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   2570\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2533\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2531\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2532\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 2533\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   2534\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   2535\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   2536\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2711\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2708\u001b[0m   cache_key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mgeneralize(cache_key)\n\u001b[0;32m   2709\u001b[0m   (args, kwargs) \u001b[39m=\u001b[39m cache_key\u001b[39m.\u001b[39m_placeholder_value()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 2711\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   2712\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39madd(cache_key, cache_key_deletion_observer,\n\u001b[0;32m   2713\u001b[0m                          graph_function)\n\u001b[0;32m   2715\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2627\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2622\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   2623\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   2624\u001b[0m ]\n\u001b[0;32m   2625\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   2626\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 2627\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   2628\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   2629\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   2630\u001b[0m         args,\n\u001b[0;32m   2631\u001b[0m         kwargs,\n\u001b[0;32m   2632\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   2633\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   2634\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   2635\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   2636\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   2637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   2638\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   2639\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   2640\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   2641\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   2642\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   2643\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   2644\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1145\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1141\u001b[0m   func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39mfunc_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1143\u001b[0m   \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m   \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m-> 1145\u001b[0m   func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[0;32m   1146\u001b[0m       convert, func_outputs, expand_composites\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1148\u001b[0m   check_func_mutation(func_args_before, func_kwargs_before, func_args,\n\u001b[0;32m   1149\u001b[0m                       func_kwargs, original_func)\n\u001b[0;32m   1150\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1104\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.convert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1097\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1098\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo be compatible with tf.function, Python functions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1099\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmust return zero or more Tensors or ExtensionTypes or None \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1100\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalues; in compilation of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(python_func)\u001b[39m}\u001b[39;00m\u001b[39m, found return \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvalue of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, which is not a Tensor or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1102\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExtensionType.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1103\u001b[0m \u001b[39mif\u001b[39;00m add_control_dependencies:\n\u001b[1;32m-> 1104\u001b[0m   x \u001b[39m=\u001b[39m deps_ctx\u001b[39m.\u001b[39;49mmark_as_return(x)\n\u001b[0;32m   1105\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:249\u001b[0m, in \u001b[0;36mAutomaticControlDependencies.mark_as_return\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    244\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor_array_ops\u001b[39m.\u001b[39mbuild_ta_with_new_flow(tensor, flow)\n\u001b[0;32m    245\u001b[0m \u001b[39m# We want to make the return values depend on the stateful operations, but\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39m# we don't want to introduce a cycle, so we make the return value the result\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39m# of a new identity operation that the stateful operations definitely don't\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[39m# depend on.\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m tensor \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39;49midentity(tensor)\n\u001b[0;32m    250\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_returned_tensors\u001b[39m.\u001b[39madd(tensor)\n\u001b[0;32m    251\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:295\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgraph\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    292\u001b[0m   \u001b[39m# Make sure we get an input with handle data attached from resource\u001b[39;00m\n\u001b[0;32m    293\u001b[0m   \u001b[39m# variables. Variables have correct handle data when graph building.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m   \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\u001b[39minput\u001b[39m)\n\u001b[1;32m--> 295\u001b[0m ret \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49midentity(\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    296\u001b[0m \u001b[39m# Propagate handle data for happier shape inference for resource variables.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39minput\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_handle_data\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:4076\u001b[0m, in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4074\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m   4075\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m-> 4076\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m   4077\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mIdentity\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   4078\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m   4079\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:797\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    792\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[0;32m    793\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[0;32m    794\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[0;32m    795\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[0;32m    796\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    798\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m    799\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m    801\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[0;32m    805\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:694\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    692\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    693\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 694\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    695\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    696\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3754\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3751\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3752\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3753\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3754\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[0;32m   3755\u001b[0m       node_def,\n\u001b[0;32m   3756\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3757\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3758\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3759\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3760\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3761\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3762\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   3763\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3764\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2129\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2127\u001b[0m   \u001b[39mif\u001b[39;00m op_def \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2128\u001b[0m     op_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39m_get_op_def(node_def\u001b[39m.\u001b[39mop)\n\u001b[1;32m-> 2129\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op \u001b[39m=\u001b[39m _create_c_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, node_def, inputs,\n\u001b[0;32m   2130\u001b[0m                             control_input_ops, op_def)\n\u001b[0;32m   2131\u001b[0m   name \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mname)\n\u001b[0;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traceback \u001b[39m=\u001b[39m tf_stack\u001b[39m.\u001b[39mextract_stack_for_node(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\activity\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1933\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1931\u001b[0m inputs \u001b[39m=\u001b[39m _reconstruct_sequence_inputs(op_def, inputs, node_def\u001b[39m.\u001b[39mattr)\n\u001b[0;32m   1932\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1933\u001b[0m op_desc \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_NewOperation(graph\u001b[39m.\u001b[39;49m_c_graph,\n\u001b[0;32m   1934\u001b[0m                                             compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mop),\n\u001b[0;32m   1935\u001b[0m                                             compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1936\u001b[0m \u001b[39mif\u001b[39;00m node_def\u001b[39m.\u001b[39mdevice:\n\u001b[0;32m   1937\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Dqn_agent()\n",
    "sc_hist = []\n",
    "st_hist = []\n",
    "pop_hist = []\n",
    "\n",
    "for e in range(1000):\n",
    "    state = DATUM_STATE\n",
    "    steps = 0\n",
    "\n",
    "    if e % 10 == 0:\n",
    "        agent.target_model.parameters = agent.train_model.parameters\n",
    "\n",
    "    while True:\n",
    "        pred_y = dnn_model.predict(state, verbose=0)[0]\n",
    "\n",
    "        action, action_index, eps = agent.act(state)\n",
    "        next_state = return_state(state, action)\n",
    "\n",
    "        reward = return_reward(next_state, pred_y)\n",
    "\n",
    "        agent.memorize(state, action, action_index, reward, next_state, steps)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 100 == 0:\n",
    "            print(f\"steps: {steps}, reward: {np.round(reward, 6)}, pop: {np.round(pred_y, 6)}\")\n",
    "\n",
    "        if steps == EPISODE_DONE:\n",
    "            print(\"=============episode done=============\")\n",
    "            print(f\"=========={e}, {eps:.6f}==========\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('activity')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b1c3e6de7689e7bdbbc50db021b43d4c3aaa0658b374e4aac167e3aa46f0f50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
