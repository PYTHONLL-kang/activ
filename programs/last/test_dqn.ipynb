{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import math\n",
    "\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\code\\\\activ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = np.array([0 for i in range(21)]).reshape(1, 21)\n",
    "goal[0][0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.array([0 for i in range(21)]).reshape(1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(goal)\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn paramater\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 64\n",
    "TRAIN_FLAG = 4000\n",
    "EPISODE_DONE = 100\n",
    "EPS_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_action(i):\n",
    "    a = np.zeros((1, 21))\n",
    "    j = i // 2\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        a[0][j] = -0.1\n",
    "    \n",
    "    else:\n",
    "        a[0][j] = 0.1\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(s, a):\n",
    "    ns = s + a\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(ns, gs):\n",
    "    dist = np.sqrt(np.sum(np.square(gs - ns)))\n",
    "    loss = dist\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Network(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN_Network, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.Dense(128, input_shape=(21, ), activation='relu')\n",
    "\n",
    "        self.hidden_layer = tf.keras.models.Sequential()\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "        self.ouput_layer = tf.keras.layers.Dense(42, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        i = self.input_layer(x)\n",
    "        h = self.hidden_layer(i)\n",
    "        o = self.ouput_layer(h)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self):\n",
    "        self.train_model = self.set_model()\n",
    "        self.target_model = self.set_model()\n",
    "\n",
    "        self.p_memory = deque(maxlen=100000)\n",
    "        self.n_memory = deque(maxlen=100000)\n",
    "        self.episode = 1\n",
    "\n",
    "        self.optim = tf.keras.optimizers.Adam(learning_rate=1e-10)\n",
    "        self.loss_fn = tf.keras.losses.Huber()\n",
    "\n",
    "    def set_model(self):\n",
    "        net = DQN_Network()\n",
    "        net.build(input_shape=(1, 21))\n",
    "\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=1e-10)\n",
    "        net.compile(optimizer=optim, loss=tf.keras.losses.Huber())\n",
    "        return net\n",
    "\n",
    "    def update_model(self):\n",
    "        self.target_model.set_weights(self.train_model.get_weights())\n",
    "\n",
    "    def memorize(self, cs, a_i, r, ns, d, sign):\n",
    "        if d:\n",
    "            self.episode += 1\n",
    "\n",
    "        if sign == 0:\n",
    "            self.n_memory.append(\n",
    "                (\n",
    "                    tf.convert_to_tensor(tf.cast(cs, tf.float32)),\n",
    "                    a_i,\n",
    "                    tf.convert_to_tensor(tf.cast(r, tf.float32)),\n",
    "                    tf.convert_to_tensor(tf.cast(ns, tf.float32)),\n",
    "                    d\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if sign == 1:\n",
    "            self.p_memory.append(\n",
    "                (\n",
    "                    tf.convert_to_tensor(tf.cast(cs, tf.float32)),\n",
    "                    a_i,\n",
    "                    tf.convert_to_tensor(tf.cast(r, tf.float32)),\n",
    "                    tf.convert_to_tensor(tf.cast(ns, tf.float32)),\n",
    "                    d\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def convert_memory_to_input(self):\n",
    "        n_batch = rand.sample(self.n_memory, BATCH_SIZE/2)\n",
    "        p_batch = rand.sample(self.p_memory, BATCH_SIZE/2)\n",
    "\n",
    "        batch = [sum(n_batch, []), sum(p_batch, [])]\n",
    "        s, a_i, r, ns, d = zip(*batch)\n",
    "\n",
    "        states = tf.convert_to_tensor(s).reshape(BATCH_SIZE, 21)\n",
    "        action_indexs = tf.convert_to_tensor(a_i)\n",
    "        rewards = tf.convert_to_tensor(r)\n",
    "        next_states = tf.convert_to_tensor(ns).reshape(BATCH_SIZE, 21)\n",
    "        dones = tf.convert_to_tensor(d)\n",
    "\n",
    "        return states, action_indexs, rewards, next_states, dones\n",
    "\n",
    "    def act(self, state):\n",
    "        # if self.episode >= 0 and self.episode < 20:\n",
    "        #     eps_threshold = 0.991 ** self.episode\n",
    "        # else:\n",
    "        #     eps_threshold = EPS_DECAY ** self.episode\n",
    "\n",
    "        eps_threshold = 0.05 + (1 - 0.05) * math.exp(-1. * self.episode / 100)\n",
    "\n",
    "        a_r = np.array(self.train_model(state))[0]\n",
    "\n",
    "        if rand.random() > eps_threshold:\n",
    "            a_i = np.argmin(a_r)\n",
    "            c = 1\n",
    "\n",
    "        else:\n",
    "            a_i = rand.randint(0, 41)\n",
    "            c = 0\n",
    "\n",
    "        a = return_action(a_i)\n",
    "\n",
    "        return a, a_i, c, eps_threshold\n",
    "\n",
    "    def run(self):\n",
    "        if len(self.n_memory) < TRAIN_FLAG or len(self.p_memory) < TRAIN_FLAG:\n",
    "            return 1\n",
    "\n",
    "        states, action_indexs, rewards, next_states, dones = self.convert_memory_to_input()\n",
    "        loss = self.learn(states, action_indexs, rewards, next_states, dones)\n",
    "    \n",
    "        return loss.numpy()\n",
    "        \n",
    "    @tf.function\n",
    "    def learn(self, states, action_indexs, rewards, next_states, dones):\n",
    "        q_target = self.target_model(next_states)\n",
    "        target_q = rewards + (1 - dones) * GAMMA * tf.reduce_min(q_target, axis=1, keepdims=True)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_q = self.train_model(states) # 현재 상황에서 할 수 있는 행동들의 q value\n",
    "            current_q = tf.reduce_sum(current_q[action_indexs], axis=1, keepdims=True) # 실제 한 행동에 대한 q value\n",
    "\n",
    "            loss = self.loss_fn(current_q, target_q)\n",
    "\n",
    "        grads = tape.gradient(loss, self.train_model.trainable_weights)\n",
    "        self.optim.apply_gradients(zip(grads, self.train_model.trainable_weights))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===update===\n",
      "=============0=============\n",
      "rewards: 2335.212, net_loss: 1, number of most decision: 35, desicion tendecy: 11, eps: 0.99055\n",
      "3.606937759374287\n",
      "=============1=============\n",
      "rewards: 2495.564, net_loss: 1, number of most decision: 37, desicion tendecy: 15, eps: 0.98119\n",
      "3.769615364994153\n",
      "=============2=============\n",
      "rewards: 2855.859, net_loss: 1, number of most decision: 43, desicion tendecy: 24, eps: 0.97192\n",
      "4.661544808322667\n",
      "=============3=============\n",
      "rewards: 2138.7, net_loss: 2.937000036239624, number of most decision: 42, desicion tendecy: 38, eps: 0.96275\n",
      "3.410278580995987\n",
      "=============4=============\n",
      "rewards: 2632.747, net_loss: 3.009000062942505, number of most decision: 39, desicion tendecy: 55, eps: 0.95367\n",
      "4.0853396431631\n",
      "=============5=============\n",
      "rewards: 2690.365, net_loss: 2.9489998817443848, number of most decision: 39, desicion tendecy: 51, eps: 0.94468\n",
      "4.419275958796872\n",
      "=============6=============\n",
      "rewards: 2836.938, net_loss: 2.8350000381469727, number of most decision: 52, desicion tendecy: 59, eps: 0.93577\n",
      "4.629254799641084\n",
      "=============7=============\n",
      "rewards: 3919.622, net_loss: 2.819000005722046, number of most decision: 52, desicion tendecy: 73, eps: 0.92696\n",
      "5.3581713298475275\n",
      "=============8=============\n",
      "rewards: 4146.773, net_loss: 3.38100004196167, number of most decision: 79, desicion tendecy: 94, eps: 0.91823\n",
      "6.771262806892074\n",
      "=============9=============\n",
      "rewards: 4275.707, net_loss: 3.6600000858306885, number of most decision: 95, desicion tendecy: 96, eps: 0.9096\n",
      "8.120960534321043\n",
      "=============10=============\n",
      "rewards: 4231.067, net_loss: 3.934000015258789, number of most decision: 92, desicion tendecy: 100, eps: 0.90104\n",
      "8.942594701763012\n",
      "=============11=============\n",
      "rewards: 5529.643, net_loss: 3.875, number of most decision: 119, desicion tendecy: 105, eps: 0.89257\n",
      "10.077201992616782\n",
      "=============12=============\n",
      "rewards: 4809.596, net_loss: 3.7660000324249268, number of most decision: 66, desicion tendecy: 112, eps: 0.88419\n",
      "7.672678802087313\n",
      "=============13=============\n",
      "rewards: 6031.8, net_loss: 4.813000202178955, number of most decision: 108, desicion tendecy: 115, eps: 0.87589\n",
      "10.08414597276337\n",
      "=============14=============\n",
      "rewards: 6233.044, net_loss: 4.144999980926514, number of most decision: 132, desicion tendecy: 143, eps: 0.86767\n",
      "11.71110584018433\n",
      "=============15=============\n",
      "rewards: 5747.35, net_loss: 5.047999858856201, number of most decision: 102, desicion tendecy: 140, eps: 0.85954\n",
      "10.795832529267937\n",
      "=============16=============\n",
      "rewards: 8162.26, net_loss: 4.539999961853027, number of most decision: 176, desicion tendecy: 172, eps: 0.85148\n",
      "15.821820375671024\n",
      "=============17=============\n",
      "rewards: 5014.297, net_loss: 3.934000015258789, number of most decision: 123, desicion tendecy: 144, eps: 0.84351\n",
      "10.71960820179542\n",
      "=============18=============\n",
      "rewards: 6510.63, net_loss: 5.466000080108643, number of most decision: 114, desicion tendecy: 164, eps: 0.83561\n",
      "12.332477447779887\n",
      "=============19=============\n",
      "rewards: 6327.997, net_loss: 5.302000045776367, number of most decision: 122, desicion tendecy: 169, eps: 0.82779\n",
      "12.457527844640746\n",
      "=============20=============\n",
      "rewards: 6544.034, net_loss: 5.785999774932861, number of most decision: 105, desicion tendecy: 172, eps: 0.82006\n",
      "10.626852779633287\n",
      "=============21=============\n",
      "rewards: 7165.972, net_loss: 5.755000114440918, number of most decision: 160, desicion tendecy: 175, eps: 0.81239\n",
      "14.367672045254897\n",
      "=============22=============\n",
      "rewards: 8167.223, net_loss: 5.803999900817871, number of most decision: 157, desicion tendecy: 200, eps: 0.80481\n",
      "14.65639792036226\n",
      "=============23=============\n",
      "rewards: 8525.783, net_loss: 5.98199987411499, number of most decision: 167, desicion tendecy: 197, eps: 0.7973\n",
      "16.36123467223665\n",
      "=============24=============\n",
      "rewards: 9087.685, net_loss: 6.390999794006348, number of most decision: 184, desicion tendecy: 216, eps: 0.78986\n",
      "17.59403307942778\n",
      "=============25=============\n",
      "rewards: 8983.091, net_loss: 7.3420000076293945, number of most decision: 184, desicion tendecy: 221, eps: 0.7825\n",
      "17.769355643916835\n",
      "=============26=============\n",
      "rewards: 7975.521, net_loss: 5.875, number of most decision: 134, desicion tendecy: 235, eps: 0.77521\n",
      "15.869782607206657\n",
      "=============27=============\n",
      "rewards: 7801.077, net_loss: 6.7179999351501465, number of most decision: 143, desicion tendecy: 201, eps: 0.76799\n",
      "14.878507989714532\n",
      "=============28=============\n",
      "rewards: 9160.489, net_loss: 7.5879998207092285, number of most decision: 200, desicion tendecy: 224, eps: 0.76085\n",
      "18.82949813457596\n",
      "=============29=============\n",
      "rewards: 9309.442, net_loss: 7.109000205993652, number of most decision: 185, desicion tendecy: 244, eps: 0.75378\n",
      "17.524554202603802\n",
      "=============30=============\n",
      "rewards: 9913.774, net_loss: 6.872000217437744, number of most decision: 200, desicion tendecy: 259, eps: 0.74677\n",
      "19.92159632158025\n",
      "=============31=============\n",
      "rewards: 8971.351, net_loss: 7.135000228881836, number of most decision: 194, desicion tendecy: 257, eps: 0.73984\n",
      "18.495675170158002\n",
      "=============32=============\n",
      "rewards: 11475.469, net_loss: 6.355999946594238, number of most decision: 230, desicion tendecy: 255, eps: 0.73298\n",
      "22.559033667247405\n",
      "=============33=============\n",
      "rewards: 9958.552, net_loss: 7.803999900817871, number of most decision: 160, desicion tendecy: 277, eps: 0.72618\n",
      "18.939112967612775\n",
      "=============34=============\n",
      "rewards: 10442.062, net_loss: 7.214000225067139, number of most decision: 216, desicion tendecy: 279, eps: 0.71945\n",
      "21.63261426642653\n",
      "=============35=============\n",
      "rewards: 12854.466, net_loss: 8.081000328063965, number of most decision: 263, desicion tendecy: 298, eps: 0.71279\n",
      "25.64117782006131\n",
      "=============36=============\n",
      "rewards: 12172.69, net_loss: 7.090000152587891, number of most decision: 248, desicion tendecy: 302, eps: 0.7062\n",
      "23.780874668523076\n",
      "=============37=============\n",
      "rewards: 10542.147, net_loss: 7.546000003814697, number of most decision: 181, desicion tendecy: 292, eps: 0.69967\n",
      "20.29310227638934\n",
      "=============38=============\n",
      "rewards: 10361.473, net_loss: 11.34000015258789, number of most decision: 180, desicion tendecy: 280, eps: 0.6932\n",
      "19.97072857959865\n",
      "=============39=============\n",
      "rewards: 12570.513, net_loss: 7.040999889373779, number of most decision: 259, desicion tendecy: 311, eps: 0.6868\n",
      "24.439926350134588\n",
      "=============40=============\n",
      "rewards: 11713.81, net_loss: 9.996000289916992, number of most decision: 230, desicion tendecy: 309, eps: 0.68047\n",
      "23.007172794587383\n",
      "=============41=============\n",
      "rewards: 11668.979, net_loss: 8.557000160217285, number of most decision: 229, desicion tendecy: 323, eps: 0.67419\n",
      "22.741371990273617\n",
      "=============42=============\n",
      "rewards: 11307.161, net_loss: 8.545999526977539, number of most decision: 218, desicion tendecy: 322, eps: 0.66798\n",
      "21.633076526467534\n",
      "=============43=============\n",
      "rewards: 15009.431, net_loss: 9.008999824523926, number of most decision: 288, desicion tendecy: 373, eps: 0.66183\n",
      "28.422350360235964\n",
      "=============44=============\n",
      "rewards: 11507.814, net_loss: 8.656000137329102, number of most decision: 183, desicion tendecy: 304, eps: 0.65575\n",
      "22.35956171305685\n",
      "=============45=============\n",
      "rewards: 11829.606, net_loss: 9.9350004196167, number of most decision: 238, desicion tendecy: 333, eps: 0.64972\n",
      "24.122810781498952\n",
      "=============46=============\n",
      "rewards: 14023.926, net_loss: 10.531999588012695, number of most decision: 291, desicion tendecy: 364, eps: 0.64375\n",
      "29.30341277052908\n",
      "=============47=============\n",
      "rewards: 14464.183, net_loss: 9.998000144958496, number of most decision: 293, desicion tendecy: 344, eps: 0.63784\n",
      "27.78290841506709\n",
      "=============48=============\n",
      "rewards: 13873.411, net_loss: 11.626999855041504, number of most decision: 236, desicion tendecy: 388, eps: 0.632\n",
      "26.520369529853845\n",
      "=============49=============\n",
      "rewards: 15431.177, net_loss: 9.25, number of most decision: 301, desicion tendecy: 379, eps: 0.6262\n",
      "29.707069865606197\n",
      "=============50=============\n",
      "rewards: 15639.91, net_loss: 8.253000259399414, number of most decision: 313, desicion tendecy: 384, eps: 0.62047\n",
      "31.01338420746773\n",
      "=============51=============\n",
      "rewards: 17116.966, net_loss: 10.973999977111816, number of most decision: 330, desicion tendecy: 387, eps: 0.61479\n",
      "32.784295020634666\n",
      "=============52=============\n",
      "rewards: 14270.277, net_loss: 9.54800033569336, number of most decision: 285, desicion tendecy: 390, eps: 0.60917\n",
      "29.718512748790197\n",
      "=============53=============\n",
      "rewards: 15377.813, net_loss: 11.579000473022461, number of most decision: 312, desicion tendecy: 374, eps: 0.60361\n",
      "31.174829590552847\n",
      "=============54=============\n",
      "rewards: 13714.821, net_loss: 10.54699993133545, number of most decision: 291, desicion tendecy: 381, eps: 0.5981\n",
      "29.191608383232445\n",
      "=============55=============\n",
      "rewards: 13704.436, net_loss: 10.871999740600586, number of most decision: 271, desicion tendecy: 409, eps: 0.59265\n",
      "28.974989214838445\n",
      "=============56=============\n",
      "rewards: 14911.333, net_loss: 11.633999824523926, number of most decision: 305, desicion tendecy: 397, eps: 0.58725\n",
      "30.582511342268926\n",
      "=============57=============\n",
      "rewards: 15482.765, net_loss: 10.484999656677246, number of most decision: 317, desicion tendecy: 386, eps: 0.5819\n",
      "31.149799357299383\n",
      "=============58=============\n",
      "rewards: 14590.521, net_loss: 10.779000282287598, number of most decision: 280, desicion tendecy: 415, eps: 0.57661\n",
      "29.36273148056914\n",
      "=============59=============\n",
      "rewards: 17331.659, net_loss: 10.737000465393066, number of most decision: 329, desicion tendecy: 412, eps: 0.57137\n",
      "33.20195777360141\n",
      "=============60=============\n",
      "rewards: 15051.623, net_loss: 11.71500015258789, number of most decision: 251, desicion tendecy: 433, eps: 0.56618\n",
      "30.370874205396248\n",
      "=============61=============\n",
      "rewards: 16966.307, net_loss: 11.977999687194824, number of most decision: 362, desicion tendecy: 451, eps: 0.56105\n",
      "36.23465192326284\n",
      "=============62=============\n",
      "rewards: 14821.21, net_loss: 12.788999557495117, number of most decision: 254, desicion tendecy: 444, eps: 0.55596\n",
      "31.334804929981676\n",
      "=============63=============\n",
      "rewards: 16080.542, net_loss: 12.038000106811523, number of most decision: 244, desicion tendecy: 447, eps: 0.55093\n",
      "32.248875949403335\n",
      "=============64=============\n",
      "rewards: 17973.026, net_loss: 15.663999557495117, number of most decision: 357, desicion tendecy: 451, eps: 0.54594\n",
      "36.11467845627335\n",
      "=============65=============\n",
      "rewards: 19094.769, net_loss: 12.857000350952148, number of most decision: 388, desicion tendecy: 474, eps: 0.54101\n",
      "38.96575419519066\n",
      "=============66=============\n",
      "rewards: 16506.641, net_loss: 13.343000411987305, number of most decision: 243, desicion tendecy: 469, eps: 0.53612\n",
      "32.87567489801548\n",
      "=============67=============\n",
      "rewards: 17097.732, net_loss: 13.097999572753906, number of most decision: 262, desicion tendecy: 455, eps: 0.53129\n",
      "32.68409399080851\n",
      "=============68=============\n",
      "rewards: 16109.773, net_loss: 12.720999717712402, number of most decision: 244, desicion tendecy: 475, eps: 0.5265\n",
      "32.6522587273837\n",
      "=============69=============\n",
      "rewards: 20190.056, net_loss: 13.375, number of most decision: 399, desicion tendecy: 491, eps: 0.52176\n",
      "39.95509979965036\n",
      "=============70=============\n",
      "rewards: 17722.753, net_loss: 16.687999725341797, number of most decision: 352, desicion tendecy: 444, eps: 0.51706\n",
      "35.12591635815375\n",
      "=============71=============\n",
      "rewards: 18340.247, net_loss: 13.074999809265137, number of most decision: 364, desicion tendecy: 457, eps: 0.51241\n",
      "36.80991714198792\n",
      "=============72=============\n",
      "rewards: 17045.104, net_loss: 16.829999923706055, number of most decision: 276, desicion tendecy: 498, eps: 0.50781\n",
      "35.3959037178034\n",
      "=============73=============\n",
      "rewards: 20627.51, net_loss: 14.279999732971191, number of most decision: 397, desicion tendecy: 506, eps: 0.50326\n",
      "40.76039744654144\n",
      "=============74=============\n",
      "rewards: 20230.307, net_loss: 14.10200023651123, number of most decision: 404, desicion tendecy: 506, eps: 0.49875\n",
      "40.450834354806844\n",
      "=============75=============\n",
      "rewards: 19459.767, net_loss: 16.084999084472656, number of most decision: 383, desicion tendecy: 476, eps: 0.49428\n",
      "38.25950862204085\n",
      "=============76=============\n",
      "rewards: 19713.179, net_loss: 18.26099967956543, number of most decision: 404, desicion tendecy: 502, eps: 0.48986\n",
      "39.948842286104124\n",
      "=============77=============\n",
      "rewards: 19838.095, net_loss: 17.281999588012695, number of most decision: 395, desicion tendecy: 505, eps: 0.48549\n",
      "39.629155933479325\n",
      "=============78=============\n",
      "rewards: 18269.171, net_loss: 17.16900062561035, number of most decision: 282, desicion tendecy: 498, eps: 0.48115\n",
      "35.66721183383989\n",
      "=============79=============\n",
      "rewards: 18499.115, net_loss: 16.982999801635742, number of most decision: 298, desicion tendecy: 514, eps: 0.47686\n",
      "37.30026809555142\n",
      "=============80=============\n",
      "rewards: 21892.658, net_loss: 17.305999755859375, number of most decision: 421, desicion tendecy: 539, eps: 0.47262\n",
      "42.35551912088936\n",
      "=============81=============\n",
      "rewards: 21395.869, net_loss: 17.695999145507812, number of most decision: 419, desicion tendecy: 519, eps: 0.46841\n",
      "42.335918556233366\n"
     ]
    }
   ],
   "source": [
    "agent = DQN_Agent()\n",
    "rewards_hist = []\n",
    "st_hist = []\n",
    "\n",
    "for e in range(5000):\n",
    "    counter = [0 for i in range(42)]\n",
    "    state = start\n",
    "    steps = 0\n",
    "    rewards = 0\n",
    "    c = 0\n",
    "\n",
    "    if e % 200 == 0:\n",
    "        agent.update_model()\n",
    "        print(\"===update===\")\n",
    "\n",
    "    while True:\n",
    "        action, idx, t, eps = agent.act(state)\n",
    "        counter[idx] += 1\n",
    "        c += t\n",
    "        next_state = return_state(state, action)\n",
    "\n",
    "        if reward < return_reward(next_state, goal):\n",
    "            sign = 0\n",
    "        else:\n",
    "            sign = 1\n",
    "\n",
    "        reward = return_reward(next_state, goal)\n",
    "\n",
    "        if steps == EPISODE_DONE or all(state[0][i] == goal[0][i] for i in range(21)):\n",
    "            done = 1\n",
    "        else:\n",
    "            done = 0\n",
    "\n",
    "        agent.memorize(state, idx, reward, next_state, done, sign)\n",
    "        loss = agent.run()\n",
    "        \n",
    "        state = next_state\n",
    "        rewards += reward\n",
    "        steps += 1\n",
    "\n",
    "        # if steps == 1:\n",
    "        #     print(f'steps: {steps}, reward: {reward}, a: {idx}')\n",
    "\n",
    "        if done:\n",
    "            rewards_hist.append(rewards)\n",
    "            st_hist.append(state)\n",
    "            print(f'============={e}=============')\n",
    "            print(f\"rewards: {round(rewards, 3)}, net_loss: {round(loss, 3)}, number of most decision: {max(counter)}, desicion tendecy: {c}, eps: {round(eps, 5)}\")\n",
    "            print(reward)\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
