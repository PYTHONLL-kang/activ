{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\code\\\\activ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = 'nov_nine_var.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = tf.keras.models.load_model('./model/dnn.h5')\n",
    "# gan_model = tf.keras.models.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn paramater\n",
    "GAMMA = 0.9\n",
    "BATCH_SIZE = 128\n",
    "ACTION_NUM = 3\n",
    "EPISODE_DONE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set action lstm network\n",
    "for i in range(ACTION_NUM):\n",
    "    break\n",
    "    globals()[f'action_net{i}'] = tf.keras.models.load_model('./model/action_lstm{0}.h5'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./documents/' + df_name).iloc[:,1::]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(df.iloc[:,0:21])\n",
    "\n",
    "starting_state = X[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_goal(goal_df_name):\n",
    "    \"\"\" set goal destination\n",
    "    Args:\n",
    "        goal_df_name(str): df_name in documents/result/\n",
    "    Returns:\n",
    "        goal_state(ndArray, (1, 21)): the state of lowest rate in df\n",
    "    \"\"\"\n",
    "    goal_df = pd.read_excel('./documents/result/' + goal_df_name).iloc[:,1::].to_numpy()\n",
    "    index = goal_df[:,23].argmin()\n",
    "\n",
    "    goal_state = goal_df[:,0:21][index]\n",
    "\n",
    "    return goal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_action(s):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        s(ndArray, (1, 21)): the state\n",
    "    Returns:\n",
    "        a(ndArray, (ACTION_NUM, 21)): the action predicted by lstm\n",
    "    \"\"\"\n",
    "    action_list = []\n",
    "    for i in range(ACTION_NUM):\n",
    "        action_list.append(globals()[f'action_net{i}'].predict(s, verbose=0))\n",
    "\n",
    "    a = np.array(action_list)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(s, a):\n",
    "    \"\"\" return s+a, which mean next state\n",
    "    Args:\n",
    "        s(ndArray, (1, 21)): the current state\n",
    "        a(ndArray, (1, 21)): the action on the current state\n",
    "    Returns:\n",
    "        ns(ndArray, (1, 21)): the next state\n",
    "    \"\"\"\n",
    "    return s + a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(ns, gs, ap):\n",
    "    \"\"\" evaluate current action\n",
    "    Args:\n",
    "        ns(ndArray, (1, 21)): the consequence of action in the current state\n",
    "        gs(ndArray, (1, 21)): the destination\n",
    "        ap(int): reality of action possible predicted by GAN\n",
    "    Returns:\n",
    "        reward(int): distance to destination + reality of action possible\n",
    "        dist(float): distance to destination\n",
    "    \"\"\"\n",
    "    dist = np.sqrt(np.sum(np.square(gs - ns)))\n",
    "    \n",
    "    return -(ap + dist), dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Network(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN_Network, self).__init__()\n",
    "        self.input_layer = tf.keras.layers.Dense(128, activation='relu')\n",
    "\n",
    "        self.hidden_layer = tf.keras.models.Sequential()\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "        self.hidden_layer.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "        self.ouput_layer = tf.keras.layers.Dense(ACTION_NUM, activation='linear')\n",
    "\n",
    "    def call(self, x):\n",
    "        a = return_action(x)\n",
    "        i = self.input_layer(a)\n",
    "        h = self.hidden_layer(h)\n",
    "        o = self.ouput_layer(o)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    def __init__(self):\n",
    "        self.train_model = self.set_model()\n",
    "        self.target_model = self.set_model()\n",
    "\n",
    "        self.memory = deque(maxlen=20000)\n",
    "        self.episode = 1\n",
    "\n",
    "    def set_model(self):\n",
    "        net = DQN_Network()\n",
    "        net.build(input_shape=(None, 21))\n",
    "\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=1e-10)\n",
    "\n",
    "        model = net.compile(optimizer=optim, loss='mse')\n",
    "        return model\n",
    "\n",
    "    def memorize(self, cs, a, a_i, r, ns, d):\n",
    "        \"\"\" append to self.memory\n",
    "        Args:\n",
    "            cs(ndArray, (1, 21)): the current state\n",
    "            a(ndArray, (1, 21)): the action on current state\n",
    "            a_i(int): the index of the action chosen by the agent\n",
    "            r(int): reward for action in the current state\n",
    "            ns(ndArray, (1, 21)): the next state\n",
    "            d(boolean): whether to proceed with the episode\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if d:\n",
    "            self.episode += 1\n",
    "        \n",
    "        self.memory.append(\n",
    "            (\n",
    "                tf.convert_to_tensor(tf.cast(cs, tf.float32)),\n",
    "                tf.convert_to_tensor(tf.cast(a, tf.float32)),\n",
    "                a_i,\n",
    "                tf.convert_to_tensor(tf.cast(r, tf.float32)),\n",
    "                tf.convert_to_tensor(tf.cast(ns, tf.float32)),\n",
    "                d\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def convert_memory_to_input(self):\n",
    "        batch = rand.sample(self.memory, BATCH_SIZE)\n",
    "        s, a, a_i, r, ns, d = zip(*batch)\n",
    "\n",
    "        states = tf.convert_to_tensor(s).reshape(BATCH_SIZE, 21)\n",
    "        actions = tf.convert_to_tensor(a).reshape(BATCH_SIZE, 21)\n",
    "        action_indexs = tf.convert_to_tensor(a_i)\n",
    "        rewards = tf.convert_to_tensor(r)\n",
    "        next_states = tf.convert_to_tensor(ns).resahpe(BATCH_SIZE, 21)\n",
    "        dones = tf.convert_to_tensor(d)\n",
    "\n",
    "        return states, actions, action_indexs, rewards, next_states, dones\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.episode >= 0 and self.episode < 200:\n",
    "            eps_threshold = -(self.episode/1000)+1+(self.episode)*(self.episode-200)/300000\n",
    "        else:\n",
    "            eps_threshold = -(self.episode/1000)+1+(self.episode-200)*(self.episode-1000)\n",
    "\n",
    "        a = return_action(state)\n",
    "        r = self.train_model(state)\n",
    "\n",
    "        if rand.random() > eps_threshold:\n",
    "            a_i = np.argmax(r)\n",
    "        else:\n",
    "            a_i = rand.randint(0, ACTION_NUM-1)\n",
    "\n",
    "        return a[a_i], a_i, eps_threshold\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        states, actions, action_indexs, rewards, next_states, dones = self.convert_memory_to_input()\n",
    "\n",
    "        current_q = self.model(states)\n",
    "        target_q = self.target(next_states)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                next_q_value = rewards[i]\n",
    "            else:\n",
    "                next_q_value = rewards[i] + GAMMA * np.max(target_q[i])\n",
    "\n",
    "            current_q[i][action_indexs[i]] = next_q_value\n",
    "\n",
    "        self.train_model.train_on_batch(states, current_q)\n",
    "        return self.train_model.evaluate(states, current_q, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'action_net0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQN_Agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m sc_hist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m st_hist \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m, in \u001b[0;36mDQN_Agent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_model()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n",
      "Cell \u001b[1;32mIn[41], line 11\u001b[0m, in \u001b[0;36mDQN_Agent.set_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     10\u001b[0m     net \u001b[38;5;241m=\u001b[39m DQN_Network()\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m21\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     optim \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\n\u001b[0;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptim, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py:449\u001b[0m, in \u001b[0;36mModel.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    445\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mYou can only call `build()` on a model if its `call()` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    447\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mmethod accepts an `inputs` argument.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    448\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 449\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (tf\u001b[39m.\u001b[39merrors\u001b[39m.\u001b[39mInvalidArgumentError, \u001b[39mTypeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mYou cannot build your model by calling `build` \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    452\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mif your layers do not support float type inputs. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    453\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mInstead, in order to instantiate and build your \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    454\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mmodel, call your model on real tensor data (of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    455\u001b[0m                    \u001b[39m'\u001b[39m\u001b[39mthe correct dtype).\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mThe actual error from \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    456\u001b[0m                    \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`call` is: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m, in \u001b[0;36mDQN_Network.call\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 13\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layer(a)\n\u001b[0;32m     15\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(h)\n",
      "Cell \u001b[1;32mIn[33], line 10\u001b[0m, in \u001b[0;36mreturn_action\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      8\u001b[0m action_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ACTION_NUM):\n\u001b[1;32m---> 10\u001b[0m     action_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction_net\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(s, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m     12\u001b[0m a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action_list)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mKeyError\u001b[0m: 'action_net0'"
     ]
    }
   ],
   "source": [
    "agent = DQN_Agent()\n",
    "sc_hist = []\n",
    "st_hist = []\n",
    "\n",
    "goal_state = set_goal('basic.xlsx')\n",
    "for e in range(1000):\n",
    "    state = starting_state\n",
    "    steps = 0\n",
    "\n",
    "    if e % 50 == 0:\n",
    "        agent.target_model.set_weights(agent.train_model.get_weights())\n",
    "\n",
    "    while True:\n",
    "        action, idx, eps = agent.act(state)\n",
    "        ap = 0\n",
    "        # ap = gan_model.predict(action, verbose=0)\n",
    "\n",
    "        next_state = return_state(state, action)\n",
    "        reward, dist = return_reward(next_state, ap)\n",
    "\n",
    "        if steps == EPISODE_DONE or state == goal_state:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        agent.memorize(state, action, idx, reward, next_state, done)\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if steps % 10 == 0:\n",
    "            print(f'steps: {steps}, reward: {reward}, dist: {dist}')\n",
    "\n",
    "        if done:\n",
    "            print(f'============={e}=============')\n",
    "            print(reward, eps)\n",
    "            print(\"=============================\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
