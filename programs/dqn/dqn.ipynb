{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random as rand\n",
    "from random import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('aug_nine_var.xlsx')\n",
    "X = df.iloc[:,1:22]\n",
    "y = df.iloc[:,22:23].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42, shuffle=False)\n",
    "np.random.shuffle(X_train)\n",
    "np.random.shuffle(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 50\n",
    "eps_start = 0.9\n",
    "eps_end = 0.05\n",
    "eps_decay = 200\n",
    "\n",
    "gamma = 0.8 #감마는 할인계수, 에이전트가 현재를 미래보다 더 가치있게 여기는 것\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __intit__(self):\n",
    "        self.model = tf.keras.models.load_model('val_0.06_dnn.h5')\n",
    "        self.steps_done = 0\n",
    "        self.memory = deque(maxlen=1000)\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append(\n",
    "            state, action, reward, next_state\n",
    "        )\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1. *self.steps_done / eps_decay)\n",
    "        self.steps_done = self.steps_done + 1\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading... try: 0 epsilon: 1.0000 y_result: 40.5117\n",
      "[ 9.7700000e-01  5.1824477e+07  9.7740880e+06  1.3061074e+07\n",
      "  2.9538440e+06  2.5789006e+07  2.5301000e+04  2.4200000e+04\n",
      "  2.2801000e+04  1.0087000e+04  2.0000000e+00 -4.8000000e+00\n",
      " -5.0000000e-01  0.0000000e+00 -1.0000000e-01 -1.0000000e-01\n",
      "  9.9330000e+01  1.7500000e+00  1.1285800e+03  5.0930000e+01\n",
      "  1.8981926e+06]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\activ\\programs\\dqn\\dqn.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 102>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn.ipynb#W6sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn.ipynb#W6sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m(Q[state,action] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn.ipynb#W6sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m     Q[state, action] \u001b[39m=\u001b[39m LEARNING_RATE \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m GAMMA \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(Q[next_state, :]) \u001b[39m-\u001b[39m Q[state, action])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn.ipynb#W6sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn.ipynb#W6sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     Q[state, action] \u001b[39m=\u001b[39m Q[state, action] \u001b[39m+\u001b[39m LEARNING_RATE \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m GAMMA \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(Q[next_state, :]) \u001b[39m-\u001b[39m Q[state, action])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reward' is not defined"
     ]
    }
   ],
   "source": [
    "class enviornment():\n",
    "    def __init__(self):\n",
    "        self.last_state = 0\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    def sample(self):\n",
    "        self.last_action = randint(0,41)\n",
    "        return self.last_action\n",
    "    def step(self,action):\n",
    "        global result_board\n",
    "        global Q\n",
    "        one  = (result_board)\n",
    "        if(action < 21):\n",
    "            result_board[action] -= 0.01\n",
    "            r = action\n",
    "\n",
    "        else:\n",
    "            result_board[action-21] += 0.01\n",
    "            r = action-21\n",
    "        if(result_board[r] < 0 or result_board[r] > 1):\n",
    "            self.last_state = self.state\n",
    "        if(action < 21):\n",
    "            result_board[action] += 0.01\n",
    "        else:\n",
    "            result_board[action-21] -= 0.01\n",
    "\n",
    "        next = result_board\n",
    "        n = False\n",
    "        rt = 0\n",
    "        global result_boards\n",
    "\n",
    "        for tt in range(0,len(result_boards)):\n",
    "            c = result_boards[tt]\n",
    "            c = list(np.round(c,6))\n",
    "            next = list(np.round(next,6))\n",
    "\n",
    "            if(list(c) == list(next)):\n",
    "                n = True\n",
    "                break\n",
    "        if(n):\n",
    "            n = False\n",
    "            if(rt == 1):\n",
    "                self.last_state = self.state\n",
    "                rt == 0\n",
    "      # print(\"rebuilding..! state: %d - %d\" % (result_boards.index(c),self.state + 1))\n",
    "            next_state = tt\n",
    "        else:\n",
    "            rt = 1\n",
    "            result_boards.append(next)\n",
    "            Q = np.concatenate([Q,np.zeros((1,42))],axis = 0)\n",
    "            next_state = self.state + 1\n",
    "            self.state += 1\n",
    "            self.last_state = self.state\n",
    "\n",
    "        c = evaluate_loss(next)\n",
    "        return next_state,c,False,None\n",
    "  \n",
    "    def read(self,next_state):\n",
    "        self.state = next_state\n",
    "#evaluate_loss의 경우 공식을 적용후 결과를 내뱉는 함수.\n",
    "    def evaluate_loss(y_pred):\n",
    "        global max_loss\n",
    "        global model\n",
    "        global X_test\n",
    "        global y_test\n",
    "        y_pred = np.array(y_pred).reshape((1,21,))\n",
    "        last_data = X_test[-1]\n",
    "        pred_1 = model.predict(y_pred,verbose=0)\n",
    "        b = np.sqrt(np.sum(np.square(y_pred-last_data)))\n",
    "        a = (np.square(y_test[-1]-pred_1))\n",
    "        return max_loss-1*(pred_1*10+a+(np.abs(b*10-a)))\n",
    "#result_board 초기화'\n",
    "global max_loss\n",
    "max_loss = 282439870\n",
    "global result_board\n",
    "result_board = X_test[-1]\n",
    "#result_board의 리스트 초기화\n",
    "#state 관리용\n",
    "global result_boards\n",
    "result_boards = []\n",
    "result_boards.append(list(result_board))\n",
    "env = enviornment()\n",
    "STATES = 2\n",
    "ACTIONS = 42\n",
    "\n",
    "env.reset()\n",
    "#Q가 바로 action에 대한 reward를 지정해주는 array이다.\n",
    "global Q\n",
    "Q = np.zeros((STATES, ACTIONS))\n",
    "EPISODES = 2000 # how many times to run the enviornment from the beginning\n",
    "MAX_STEPS = 100  # max number of steps allowed for each run of enviornment\n",
    "LEARNING_RATE = 0.01  # learning rate\n",
    "GAMMA = 0.96\n",
    "min_thing = 0.1\n",
    "RENDER = False # if you want to see training set to true\n",
    "result_ = []\n",
    "epsilon = 1\n",
    "y = []\n",
    "rate = 0.9995\n",
    "rewards = []\n",
    "for episode in range(EPISODES):\n",
    "    y_result = model.predict(np.array(result_board).reshape(1,21,),verbose=0)[0][0]\n",
    "    print(\"loading... try: %d epsilon: %.4f y_result: %.4f\" % (episode,epsilon,y_result))\n",
    "    print((scaler.inverse_transform(np.array(result_board).reshape(1,21,)))[0])\n",
    "    state = env.reset()\n",
    "    result_board = X_test[-1]\n",
    "    for _ in range(MAX_STEPS):\n",
    "        if rand.random() < epsilon:\n",
    "            action = env.sample()  \n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "        if(Q[state,action] == 0):\n",
    "            Q[state, action] = LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
    "        else:\n",
    "            Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n",
    "            state = next_state\n",
    "            env.read(next_state)\n",
    "        if(state % 1000 == 0):\n",
    "            print(\"processing... state : %d\" % (state))\n",
    "            state += 1\n",
    "    if(episode % 1000 == 0):\n",
    "        print(\"learning... episode : %d\" % (episode))\n",
    "        episode += 1\n",
    "    rewards.append(reward)\n",
    "    epsilon *= rate\n",
    "    epsilon = max(epsilon,min_thing)\n",
    "    k = (scaler.inverse_transform(np.array(result_board).reshape(1,21,)))[0]\n",
    "    k = k.tolist()\n",
    "    y_result = model.predict(np.array(result_board).reshape(1,21,),verbose=0)[0][0]\n",
    "    k.append(y_result)\n",
    "    result_.append(k)\n",
    "    result_ = np.array(result_)\n",
    "    pl = pd.DataFrame(result_)\n",
    "    pl.to_excel(\"result.xlsx\")\n",
    "    result_ = list(result_)\n",
    "    y = list(y)\n",
    "    print(\"Average reward: : %d (%.4f)\" % (list(np.array(sum(rewards)/len(rewards)))[0][0],list(np.array(sum(rewards)/len(rewards)))[0][0]-max_loss))\n",
    "# and now we can see our Q values!\n",
    "\n",
    "def get_average(values):\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "avg_rewards = []\n",
    "for i in range(0, len(rewards), 100):\n",
    "    avg_rewards.append(get_average(rewards)) \n",
    "plt.plot(avg_rewards)\n",
    "plt.ylabel('average reward')\n",
    "plt.xlabel('episodes (100\\'s)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tens-cpu38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d69d666dea1d14e04e4d38b06e20d4d04fc31a418cef854ebdceb8b68a0927a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
