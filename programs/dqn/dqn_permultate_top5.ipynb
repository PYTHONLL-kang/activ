{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from collections import deque\n",
    "import random as rand\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def return_latest():\n",
    "    df = pd.read_excel('aug_nine_var.xlsx')\n",
    "    X = df.iloc[:,1:22]\n",
    "    y = df.iloc[:,22:23].to_numpy()\n",
    "\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X[-1], y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(state, y_pred):\n",
    "    x, _ = return_latest()\n",
    "    REAL_X = x.reshape(1, 21)[0]\n",
    "    \n",
    "    real = REAL_X * 10\n",
    "    state = state * 10\n",
    "    \n",
    "    d = abs(real - state)\n",
    "    \n",
    "    ds = 0\n",
    "    for i in range(21):\n",
    "        ds = ds + d[0][i]\n",
    "    \n",
    "    reward = 10000 / (ds + y_pred)\n",
    "    return reward, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(action, state):\n",
    "    if action % 2 == 0:\n",
    "        value = -0.01\n",
    "    \n",
    "    else:\n",
    "        value = 0.01\n",
    "\n",
    "    j = int(action / 2)\n",
    "    \n",
    "    state[0][j] = state[0][j] + value\n",
    "    next_state = state\n",
    "\n",
    "    return next_state, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(256, input_dim=21, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(42, activation='linear'))\n",
    "        \n",
    "        self.model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "        self.steps_done = 0 #학습 반복 시 증가\n",
    "        self.memory = deque(maxlen=1000) #deque는 선입선출. 오래된 값을 지움\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((\n",
    "                            state,\n",
    "                            action,\n",
    "                            reward,\n",
    "                            next_state\n",
    "        ))\n",
    "        #현재 상태, 현재 행동, 현재 행동으로 인한 보상, 다음 상태\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = eps_end + ((eps_start - eps_end) * math.exp(-1. * self.steps_done / eps_decay))\n",
    "        self.steps_done = self.steps_done + 1\n",
    "\n",
    "        if rand.random() > eps_threshold: #최대 보상\n",
    "            return self.model.predict(state, verbose=0).max()\n",
    "        \n",
    "        else: #무작위\n",
    "            return rand.randrange(10)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < 64:\n",
    "            return\n",
    "        \n",
    "        batch = rand.sample(self.memory, 64)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        states = tf.convert_to_tensor(states)\n",
    "        actions = tf.convert_to_tensor(actions)\n",
    "        rewards = tf.convert_to_tensor(rewards)\n",
    "        next_states = tf.convert_to_tensor(next_states)\n",
    "\n",
    "        states = tf.reshape(states, [len(states), 21])\n",
    "\n",
    "        # print(\"states: {0}, actions: {1}, rewards: {2}, next_states: {3}\".format(type(states), type(actions), type(rewards), type(next_states)))\n",
    "        print(\"states: {0}, actions: {1}, rewards: {2}, next_states: {3}\".format(states.shape, actions.shape, rewards.shape, next_states.shape))\n",
    "\n",
    "        current_q = self.model.predict(states, verbose=0).gather(1, actions) # 64, 1. 그때 그때 한 행동들의 가치\n",
    "        \n",
    "#         print(\"cq: {0}, mq: {1}, eq: {2}, re: {3}\".format(current_q.shape, max_next_q.shape, expected_q.shape, rewards.shape))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1000\n",
    "episode_step = 1000\n",
    "eps_start = 0.9 #학습 시작 시 무작위 행동할 확률\n",
    "eps_end = 0.05 #학습 종료 시 무작위 행동할 확률\n",
    "eps_decay = 100 #학습이 반복되며 무작위로 행동할 확률 감소 값\n",
    "\n",
    "#eps = epsilion\n",
    "\n",
    "gamma = 0.8 #감마는 할인계수, 에이전트가 현재를 미래보다 더 가치있게 여기는 것\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "model = tf.keras.models.load_model('dnn.h5')\n",
    "agent = DqnAgent()\n",
    "\n",
    "score_history = []\n",
    "state_history = []\n",
    "predict_history = []\n",
    "\n",
    "best_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, ds: 0.1, y_pred: 42, j: 2, reward: 237.52969121140143\n",
      "step: 10, ds: 0.8999999999999984, y_pred: 42, j: 0, reward: 233.1002331002331\n",
      "step: 20, ds: 1.3, y_pred: 42, j: 0, reward: 230.9468822170901\n",
      "step: 30, ds: 1.5000000000000007, y_pred: 42, j: 3, reward: 229.88505747126436\n",
      "step: 40, ds: 2.0999999999999988, y_pred: 42, j: 2, reward: 226.75736961451247\n",
      "step: 50, ds: 2.700000000000002, y_pred: 42, j: 3, reward: 223.71364653243847\n",
      "step: 60, ds: 3.300000000000002, y_pred: 42, j: 0, reward: 220.75055187637966\n",
      "states: (64, 21), actions: (64,), rewards: (64,), next_states: (64, 1, 21)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'gather'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\activ\\programs\\dqn\\dqn_permultate_top5.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m reward, ds \u001b[39m=\u001b[39m return_reward(state, predict_y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m agent\u001b[39m.\u001b[39mmemorize(state, action, reward, next_state)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m a \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m steps \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstep: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, ds: \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m, y_pred: \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m, j: \u001b[39m\u001b[39m{3}\u001b[39;00m\u001b[39m, reward: \u001b[39m\u001b[39m{4}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(steps, ds, predict_y, j, reward))\n",
      "\u001b[1;32mc:\\code\\activ\\programs\\dqn\\dqn_permultate_top5.ipynb 셀 7\u001b[0m in \u001b[0;36mDqnAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m         \u001b[39m# print(\"states: {0}, actions: {1}, rewards: {2}, next_states: {3}\".format(type(states), type(actions), type(rewards), type(next_states)))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstates: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m, actions: \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m, rewards: \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m, next_states: \u001b[39m\u001b[39m{3}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(states\u001b[39m.\u001b[39mshape, actions\u001b[39m.\u001b[39mshape, rewards\u001b[39m.\u001b[39mshape, next_states\u001b[39m.\u001b[39mshape))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         current_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(states, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mgather(\u001b[39m1\u001b[39m, actions) \u001b[39m# 64, 1. 그때 그때 한 행동들의 가치\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m#         print(\"cq: {0}, mq: {1}, eq: {2}, re: {3}\".format(current_q.shape, max_next_q.shape, expected_q.shape, rewards.shape))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'gather'"
     ]
    }
   ],
   "source": [
    "for e in range(1, episode+1):\n",
    "    state, _ = return_latest()\n",
    "    state = state.reshape(1, 21)\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        predict_y = model.predict(state, verbose=0)\n",
    "        predict_y = predict_y.astype(int)\n",
    "        predict_y = predict_y[0][0]\n",
    "\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, j = return_state(action, state)\n",
    "        reward, ds = return_reward(state, predict_y)\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        a = agent.learn()\n",
    "        \n",
    "        if steps % 10 == 0:\n",
    "            print(\"step: {0}, ds: {1}, y_pred: {2}, j: {3}, reward: {4}\".format(steps, ds, predict_y, j, reward))\n",
    "        \n",
    "        if (best_reward < reward):\n",
    "            best_reward = reward\n",
    "            best_state = state\n",
    "\n",
    "        state = next_state\n",
    "        steps = steps + 1\n",
    "\n",
    "        if steps == episode_step:\n",
    "            print(\"=============episode done=============\")\n",
    "            print(\"episode: {0}, y_pred: {1}, score: {2}\".format(e, predict_y, reward))\n",
    "            print(\"=======================================\")\n",
    "            score_history.append(reward)\n",
    "            state_history.append(scaler.inverse_transform(state))\n",
    "            predict_history.append(predict_y)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tens-cpu38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d69d666dea1d14e04e4d38b06e20d4d04fc31a418cef854ebdceb8b68a0927a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
