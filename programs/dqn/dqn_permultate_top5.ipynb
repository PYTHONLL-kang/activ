{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from collections import deque\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_latest():\n",
    "    df = pd.read_excel('aug_nine_var.xlsx')\n",
    "    X = df.iloc[:,1:22]\n",
    "    y = df.iloc[:,22:23].to_numpy()\n",
    "\n",
    "    scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    return X[-1], y[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\activ\\programs\\dqn\\dqn_permultate_top5.ipynb 셀 3\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,\u001b[39m1\u001b[39m:\u001b[39m22\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,\u001b[39m22\u001b[39m:\u001b[39m23\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m scaler \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m X \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'preprocessing'"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('aug_nine_var.xlsx')\n",
    "X = df.iloc[:,1:22]\n",
    "y = df.iloc[:,22:23].to_numpy()\n",
    "\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(state, y_pred):\n",
    "    x, _ = return_latest()\n",
    "    REAL_X = x.reshape(1, 21)[0]\n",
    "    \n",
    "    real = REAL_X * 10\n",
    "    state = state * 10\n",
    "    \n",
    "    d = abs(real - state)\n",
    "    \n",
    "    ds = 0\n",
    "    for i in range(21):\n",
    "        ds = ds + d[0][i]\n",
    "    \n",
    "    reward = 10000 / (ds + y_pred)\n",
    "    return reward, ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(action, state):\n",
    "    if action % 2 == 0:\n",
    "        value = -0.01\n",
    "    \n",
    "    else:\n",
    "        value = 0.01\n",
    "\n",
    "    j = int(action / 2)\n",
    "    \n",
    "    state[0][j] = state[0][j] + value\n",
    "    next_state = state\n",
    "\n",
    "    return next_state, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent:\n",
    "    def __init__(self):\n",
    "        self.model = tf.keras.models.Sequential()\n",
    "        self.model.add(tf.keras.layers.Dense(256, input_dim=21, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "        self.model.add(tf.keras.layers.Dense(10, activation='linear'))\n",
    "\n",
    "        self.steps_done = 0 #학습 반복 시 증가\n",
    "        self.memory = deque(maxlen=1000) #deque는 선입선출. 오래된 값을 지움\n",
    "    \n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((\n",
    "                            state,\n",
    "                            action,\n",
    "                            reward,\n",
    "                            next_state\n",
    "        ))\n",
    "        #현재 상태, 현재 행동, 현재 행동으로 인한 보상, 다음 상태\n",
    "    \n",
    "    def act(self, state):\n",
    "        eps_threshold = eps_end + ((eps_start - eps_end) * math.exp(-1. * self.steps_done / eps_decay))\n",
    "        self.steps_done = self.steps_done + 1\n",
    "\n",
    "        if rand.random() > eps_threshold: #최대 보상\n",
    "            return self.model.predict(state).max()\n",
    "        \n",
    "        else: #무작위\n",
    "            return rand.randrange(10)\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.memory) < 64:\n",
    "            return\n",
    "        \n",
    "        batch = rand.sample(self.memory, 64)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        print(\"states: {0}, actions: {1}, rewards: {2}, next_states: {3}\".format(states.shape, actions.shape, rewards.shape, next_states))\n",
    "\n",
    "        current_q = self.model.predicts(states).gather(1, actions) # 64, 1. 그때 그때 한 행동들의 가치\n",
    "        \n",
    "#         print(\"cq: {0}, mq: {1}, eq: {2}, re: {3}\".format(current_q.shape, max_next_q.shape, expected_q.shape, rewards.shape))\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 1000\n",
    "episode_step = 1000\n",
    "eps_start = 0.9 #학습 시작 시 무작위 행동할 확률\n",
    "eps_end = 0.05 #학습 종료 시 무작위 행동할 확률\n",
    "eps_decay = 100 #학습이 반복되며 무작위로 행동할 확률 감소 값\n",
    "\n",
    "#eps = epsilion\n",
    "\n",
    "gamma = 0.8 #감마는 할인계수, 에이전트가 현재를 미래보다 더 가치있게 여기는 것\n",
    "\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "model = tf.keras.models.load_model('dnn.h5')\n",
    "agent = DqnAgent()\n",
    "\n",
    "score_history = []\n",
    "state_history = []\n",
    "predict_history = []\n",
    "\n",
    "best_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\code\\activ\\programs\\dqn\\dqn_permultate_top5.ipynb 셀 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, episode\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     state, _ \u001b[39m=\u001b[39m return_latest()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m21\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;32mc:\\code\\activ\\programs\\dqn\\dqn_permultate_top5.ipynb 셀 7\u001b[0m in \u001b[0;36mreturn_latest\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,\u001b[39m1\u001b[39m:\u001b[39m22\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:,\u001b[39m22\u001b[39m:\u001b[39m23\u001b[39m]\u001b[39m.\u001b[39mto_numpy()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m scaler \u001b[39m=\u001b[39m sklearn\u001b[39m.\u001b[39;49mpreprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m X \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/code/activ/programs/dqn/dqn_permultate_top5.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], y[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'preprocessing'"
     ]
    }
   ],
   "source": [
    "for e in range(1, episode+1):\n",
    "    state, _ = return_latest()\n",
    "    state = state.reshape(1, 21)\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        predict_y = model.predict(state, verbose=0)\n",
    "        predict_y = predict_y.astype(int)\n",
    "        predict_y = predict_y[0][0]\n",
    "\n",
    "        action = agent.act(state)\n",
    "\n",
    "        next_state, j = return_state(action, state)\n",
    "        reward, ds = return_reward(state, predict_y)\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state)\n",
    "        a = agent.learn()\n",
    "        \n",
    "        if steps % 10 == 0:\n",
    "            print(\"step: {0}, ds: {1}, y_pred: {2}, j: {3}, reward: {4}\".format(steps, ds, predict_y, j, reward))\n",
    "        \n",
    "        if (best_reward < reward):\n",
    "            best_reward = reward\n",
    "            best_state = state\n",
    "\n",
    "        state = next_state\n",
    "        steps = steps + 1\n",
    "\n",
    "        if steps == episode_step:\n",
    "            print(\"=============episode done=============\")\n",
    "            print(\"episode: {0}, y_pred: {1}, score: {2}\".format(e, predict_y, reward))\n",
    "            print(\"=======================================\")\n",
    "            score_history.append(reward)\n",
    "            state_history.append(scaler.inverse_transform(state))\n",
    "            predict_history.append(predict_y)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tens-cpu38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d69d666dea1d14e04e4d38b06e20d4d04fc31a418cef854ebdceb8b68a0927a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
