{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "import math\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data = pd.read_excel('./documents/nov_nine_var.xlsx').to_numpy()\n",
    "goal_data = pd.read_excel('./documents/result/basic_formula.xlsx').to_numpy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler = scaler.fit(real_data[:,1:22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(l):\n",
    "    return max(range(len(l)), key=lambda i: l[i])\n",
    "\n",
    "def argmin(l):\n",
    "    return min(range(len(l)), key=lambda i: l[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51 0.5  0.5  0.49 0.5  0.5  0.5  0.5  0.5  0.5  0.49 0.5  0.5  0.5\n",
      " 0.5  0.5  0.49 0.49 0.51 0.5  0.5 ]\n",
      "[0.   0.98 0.   1.   0.99 1.   0.   0.78 0.09 0.46 0.4  0.58 0.27 0.32\n",
      " 0.23 0.23 1.   0.03 0.5  0.43 1.  ]\n",
      "689\n"
     ]
    }
   ],
   "source": [
    "start = np.round(scaler.transform(real_data[:,1:22])[-1].reshape(1, 21), 2)\n",
    "goal = np.round(scaler.transform(goal_data[:,1:22])[argmin(goal_data[:,-1])].reshape(1, 21), 2)\n",
    "\n",
    "print(goal[0])\n",
    "print(start[0])\n",
    "\n",
    "need_step = int(np.sum(abs(goal-start))*100)\n",
    "print(need_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state = np.array([start, goal]).reshape(1, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor-critic hyperparmater\n",
    "GAMMA = 0.99\n",
    "EPISODE_DONE = need_step * 10\n",
    "LEARN_FREQ = 1\n",
    "ACTION_NUM = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_action(i):\n",
    "    a = np.zeros((1, 21))\n",
    "    j = i // 2\n",
    "\n",
    "    if i % 2 == 0:\n",
    "        a[0][j] = -0.01\n",
    "    \n",
    "    else:\n",
    "        a[0][j] = 0.01\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state(s, a):\n",
    "    ns = s + a\n",
    "    return ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_reward(ns, gs):\n",
    "    dist = np.sqrt(np.sum(np.square(gs - ns)))\n",
    "\n",
    "    end = 0\n",
    "    for i in range(21):\n",
    "        if ns[0][i] == gs[0][i]:\n",
    "            end += 5\n",
    "    \n",
    "    reward = -dist + end\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class critic(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.v = tf.keras.layers.Dense(1, activation = None)\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    v = self.v(x)\n",
    "    return v\n",
    "    \n",
    "\n",
    "class actor(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.d1 = tf.keras.layers.Dense(128,activation='relu')\n",
    "    self.a = tf.keras.layers.Dense(2,activation='softmax')\n",
    "\n",
    "  def call(self, input_data):\n",
    "    x = self.d1(input_data)\n",
    "    a = self.a(x)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.gamma = GAMMA\n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.c_opt = tf.keras.optimizers.Adam(learning_rate=7e-3)\n",
    "        self.actor = actor()\n",
    "        self.critic = critic()\n",
    "        self.clip_pram = 0.2\n",
    "\n",
    "    def act(self,state):\n",
    "        prob = self.actor(np.array([state]))\n",
    "        prob_np = prob.numpy()\n",
    "        dist = tfp.distributions.Categorical(probs=prob_np, dtype=tf.float32)\n",
    "        action = dist.sample()\n",
    "        return int(action.numpy()[0][0]), prob[0]\n",
    "\n",
    "    def actor_loss(self, probs, actions, adv, old_probs, closs):\n",
    "        probability = probs\n",
    "        entropy = tf.reduce_mean(tf.math.negative(tf.math.multiply(probability,tf.math.log(probability))))\n",
    "\n",
    "        sur1 = []\n",
    "        sur2 = []\n",
    "\n",
    "        for pb, t, op,a  in zip(probability, adv, old_probs, actions):\n",
    "                        t =  tf.constant(t)\n",
    "                        ratio = tf.math.divide(pb[a],op[a])\n",
    "                        s1 = tf.math.multiply(ratio,t)\n",
    "                        s2 =  tf.math.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_pram, 1.0 + self.clip_pram),t)\n",
    "                        sur1.append(s1)\n",
    "                        sur2.append(s2)\n",
    "\n",
    "        sr1 = tf.stack(sur1)\n",
    "        sr2 = tf.stack(sur2)\n",
    "\n",
    "        loss = tf.math.negative(tf.reduce_mean(tf.math.minimum(sr1, sr2)) - closs + 0.001 * entropy)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def learn(self, states, actions,  adv , old_probs, discnt_rewards):\n",
    "        discnt_rewards = tf.reshape(discnt_rewards, (len(discnt_rewards),))\n",
    "        adv = tf.reshape(adv, (len(adv),))\n",
    "\n",
    "        old_p = old_probs\n",
    "\n",
    "        old_p = tf.reshape(old_p, (len(old_p),2))\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            p = self.actor(states, training=True)\n",
    "            v =  self.critic(states,training=True)\n",
    "            v = tf.reshape(v, (len(v),))\n",
    "            td = tf.math.subtract(discnt_rewards, v)\n",
    "            c_loss = 0.5 * tf.keras.losses.mean_squared_error(discnt_rewards, v)\n",
    "            a_loss = self.actor_loss(p, actions, adv, old_probs, c_loss)\n",
    "            \n",
    "        grads1 = tape1.gradient(a_loss, self.actor.trainable_variables)\n",
    "        grads2 = tape2.gradient(c_loss, self.critic.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.actor.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic.trainable_variables))\n",
    "        return a_loss, c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(agent):\n",
    "    total_reward = 0\n",
    "    state = start\n",
    "    done = 0\n",
    "    while not done:\n",
    "        model_state = np.array([state, goal]).reshape(1, 42)\n",
    "        a, _ = agent.act(model_state)\n",
    "        action = return_action(a)\n",
    "        next_state = return_state(state, action)\n",
    "        reward = return_reward(next_state, goal)\n",
    "        \n",
    "        if all(state[0] == goal[0]):\n",
    "            done = 1\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(states, actions, rewards, dones, values):\n",
    "    g = 0\n",
    "    lamb = 0.95\n",
    "    returns = []\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + GAMMA * values[i+1] * dones[i] - values[i]\n",
    "        g = delta + GAMMA * lamb * dones[i] * g\n",
    "        returns.append(g + values[i])\n",
    "    \n",
    "    returns.reverse()\n",
    "    adv = np.array(returns, dtype=np.float32) - values[:-1]\n",
    "    adv = (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "\n",
    "    states = np.array(states, dtype=np.float32)\n",
    "    actions = np.array(actions, dtype=np.int8)\n",
    "    returns = np.array(returns, dtype=np.float32)\n",
    "\n",
    "    return states, actions, returns, adv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: -53.97\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m states, actions, returns, adv \u001b[38;5;241m=\u001b[39m preprocess(states, actions, rewards, dones, values)\n\u001b[0;32m     11\u001b[0m al, cl \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mlearn(states, actions, adv, probs, returns)\n\u001b[1;32m---> 13\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([test_reward(agent) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)])\n\u001b[0;32m     14\u001b[0m avg_reward_list\u001b[38;5;241m.\u001b[39mappend(avg_reward)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_reward \u001b[38;5;241m>\u001b[39m best_reward:\n",
      "Cell \u001b[1;32mIn[35], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m states, actions, returns, adv \u001b[38;5;241m=\u001b[39m preprocess(states, actions, rewards, dones, values)\n\u001b[0;32m     11\u001b[0m al, cl \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mlearn(states, actions, adv, probs, returns)\n\u001b[1;32m---> 13\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean([\u001b[43mtest_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m)])\n\u001b[0;32m     14\u001b[0m avg_reward_list\u001b[38;5;241m.\u001b[39mappend(avg_reward)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_reward \u001b[38;5;241m>\u001b[39m best_reward:\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mtest_reward\u001b[1;34m(agent)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     model_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([state, goal])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     a, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     action \u001b[38;5;241m=\u001b[39m return_action(a)\n\u001b[0;32m      9\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m return_state(state, action)\n",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m, in \u001b[0;36mAgent.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     12\u001b[0m prob_np \u001b[38;5;241m=\u001b[39m prob\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m dist \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(probs\u001b[38;5;241m=\u001b[39mprob_np, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 14\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(action\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]), prob[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:1234\u001b[0m, in \u001b[0;36mDistribution.sample\u001b[1;34m(self, sample_shape, seed, name, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[39m\"\"\"Generate samples of the specified shape.\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[39mNote that a call to `sample()` without arguments will generate a single\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[39m  samples: a `Tensor` with prepended dimensions `sample_shape`.\u001b[39;00m\n\u001b[0;32m   1232\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_and_control_scope(name):\n\u001b[1;32m-> 1234\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_sample_n(sample_shape, seed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:1213\u001b[0m, in \u001b[0;36mDistribution._call_sample_n\u001b[1;34m(self, sample_shape, seed, **kwargs)\u001b[0m\n\u001b[0;32m   1209\u001b[0m sample_shape, n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_sample_shape_to_vector(\n\u001b[0;32m   1210\u001b[0m     sample_shape, \u001b[39m'\u001b[39m\u001b[39msample_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1211\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_n(\n\u001b[0;32m   1212\u001b[0m     n, seed\u001b[39m=\u001b[39mseed() \u001b[39mif\u001b[39;00m callable(seed) \u001b[39melse\u001b[39;00m seed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1213\u001b[0m samples \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[0;32m   1214\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: tf\u001b[39m.\u001b[39;49mreshape(x, ps\u001b[39m.\u001b[39;49mconcat([sample_shape, ps\u001b[39m.\u001b[39;49mshape(x)[\u001b[39m1\u001b[39;49m:]], \u001b[39m0\u001b[39;49m)),\n\u001b[0;32m   1215\u001b[0m     samples)\n\u001b[0;32m   1216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_sample_static_shape(samples, sample_shape)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    913\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    915\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 916\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    917\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py:1214\u001b[0m, in \u001b[0;36mDistribution._call_sample_n.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1209\u001b[0m sample_shape, n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_sample_shape_to_vector(\n\u001b[0;32m   1210\u001b[0m     sample_shape, \u001b[39m'\u001b[39m\u001b[39msample_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1211\u001b[0m samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_n(\n\u001b[0;32m   1212\u001b[0m     n, seed\u001b[39m=\u001b[39mseed() \u001b[39mif\u001b[39;00m callable(seed) \u001b[39melse\u001b[39;00m seed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1213\u001b[0m samples \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[1;32m-> 1214\u001b[0m     \u001b[39mlambda\u001b[39;00m x: tf\u001b[39m.\u001b[39;49mreshape(x, ps\u001b[39m.\u001b[39;49mconcat([sample_shape, ps\u001b[39m.\u001b[39;49mshape(x)[\u001b[39m1\u001b[39;49m:]], \u001b[39m0\u001b[39;49m)),\n\u001b[0;32m   1215\u001b[0m     samples)\n\u001b[0;32m   1216\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_sample_static_shape(samples, sample_shape)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:202\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmanip.reshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     67\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(tensor, shape, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[0;32m     69\u001b[0m   \u001b[39mr\u001b[39m\u001b[39m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[39m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m   result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mreshape(tensor, shape, name)\n\u001b[0;32m    203\u001b[0m   tensor_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[0;32m    204\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:8532\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   8530\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   8531\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 8532\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   8533\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mReshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, tensor, shape)\n\u001b[0;32m   8534\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   8535\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent()\n",
    "best_reward = 0\n",
    "avg_reward_list = []\n",
    "\n",
    "for e in range(10000):\n",
    "    if e % LEARN_FREQ == 0:\n",
    "        if e != 0:    \n",
    "            values.append(c[0][0])\n",
    "\n",
    "            states, actions, returns, adv = preprocess(states, actions, rewards, dones, values)\n",
    "            al, cl = agent.learn(states, actions, adv, probs, returns)\n",
    "        \n",
    "            avg_reward = np.mean([test_reward(agent) for _ in range(5)])\n",
    "            avg_reward_list.append(avg_reward)\n",
    "\n",
    "            if avg_reward > best_reward:\n",
    "                agent.actor.save('./model/ppo_actor')\n",
    "                agent.critic.save('./model/ppo_critic')\n",
    "                best_reward = avg_reward\n",
    "\n",
    "            if best_reward == 200:\n",
    "                break\n",
    "        \n",
    "        all_aloss = []\n",
    "        all_closs = []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        actions = []\n",
    "        probs = []\n",
    "        dones = []\n",
    "        values = []\n",
    "\n",
    "    state = start\n",
    "    steps = 0\n",
    "    episode_reward = 0\n",
    "\n",
    "    done = 0\n",
    "\n",
    "    while True:\n",
    "        model_state = np.array([state, goal]).reshape(1, 42)\n",
    "        a, p = agent.act(model_state)\n",
    "        c = agent.critic(model_state).numpy()\n",
    "\n",
    "        action = return_action(a)\n",
    "        next_state = return_state(state, action)\n",
    "\n",
    "        reward = return_reward(next_state, goal)\n",
    "\n",
    "        if steps == EPISODE_DONE or all(state[0] == goal[0]):\n",
    "            done = 1\n",
    "        \n",
    "        dones.append(done)\n",
    "        rewards.append(reward)\n",
    "        states.append(model_state[0])\n",
    "        actions.append(a)\n",
    "        probs.append(p[0])\n",
    "        values.append(c[0][0])\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            print(f'{e}: {round(reward, 2)}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
