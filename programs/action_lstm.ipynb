{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rand\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"C:\\\\code\\\\activ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./documents/feature.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>합계출산율</th>\n",
       "      <th>국내 총인구</th>\n",
       "      <th>서울인구</th>\n",
       "      <th>경기도인구</th>\n",
       "      <th>인천시인구</th>\n",
       "      <th>수도권 인구</th>\n",
       "      <th>신생아 출산인구</th>\n",
       "      <th>사망인구</th>\n",
       "      <th>혼인건수</th>\n",
       "      <th>...</th>\n",
       "      <th>- 농축수산물</th>\n",
       "      <th>- 공업제품</th>\n",
       "      <th>- 집세</th>\n",
       "      <th>- 공공서비스</th>\n",
       "      <th>- 개인서비스</th>\n",
       "      <th>소비자 물가 지수</th>\n",
       "      <th>금리</th>\n",
       "      <th>원 달러 환율</th>\n",
       "      <th>국제유가(WTI, 달러)</th>\n",
       "      <th>국내총생산</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1986-01-01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>41265113</td>\n",
       "      <td>9798542</td>\n",
       "      <td>5075449</td>\n",
       "      <td>1441131</td>\n",
       "      <td>16315122</td>\n",
       "      <td>69708</td>\n",
       "      <td>22924</td>\n",
       "      <td>36162</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30.117</td>\n",
       "      <td>9.70</td>\n",
       "      <td>890.53</td>\n",
       "      <td>19.66</td>\n",
       "      <td>102985.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1986-02-01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>41265113</td>\n",
       "      <td>9798542</td>\n",
       "      <td>5075449</td>\n",
       "      <td>1441131</td>\n",
       "      <td>16315122</td>\n",
       "      <td>73659</td>\n",
       "      <td>20258</td>\n",
       "      <td>36522</td>\n",
       "      <td>...</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>30.293</td>\n",
       "      <td>9.70</td>\n",
       "      <td>886.39</td>\n",
       "      <td>13.23</td>\n",
       "      <td>102985.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1986-03-01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>41265113</td>\n",
       "      <td>9798542</td>\n",
       "      <td>5075449</td>\n",
       "      <td>1441131</td>\n",
       "      <td>16315122</td>\n",
       "      <td>48593</td>\n",
       "      <td>22144</td>\n",
       "      <td>38380</td>\n",
       "      <td>...</td>\n",
       "      <td>5.8</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.1</td>\n",
       "      <td>5.2</td>\n",
       "      <td>30.353</td>\n",
       "      <td>9.70</td>\n",
       "      <td>884.37</td>\n",
       "      <td>10.40</td>\n",
       "      <td>102985.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1986-04-01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>41265113</td>\n",
       "      <td>9798542</td>\n",
       "      <td>5075449</td>\n",
       "      <td>1441131</td>\n",
       "      <td>16315122</td>\n",
       "      <td>47880</td>\n",
       "      <td>20892</td>\n",
       "      <td>36333</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.1</td>\n",
       "      <td>30.323</td>\n",
       "      <td>9.70</td>\n",
       "      <td>885.60</td>\n",
       "      <td>13.34</td>\n",
       "      <td>102985.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1986-05-01</td>\n",
       "      <td>1.58</td>\n",
       "      <td>41265113</td>\n",
       "      <td>9798542</td>\n",
       "      <td>5075449</td>\n",
       "      <td>1441131</td>\n",
       "      <td>16315122</td>\n",
       "      <td>47800</td>\n",
       "      <td>18495</td>\n",
       "      <td>37520</td>\n",
       "      <td>...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.1</td>\n",
       "      <td>30.501</td>\n",
       "      <td>9.70</td>\n",
       "      <td>887.21</td>\n",
       "      <td>14.25</td>\n",
       "      <td>102985.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>51671569</td>\n",
       "      <td>9558153</td>\n",
       "      <td>13512867</td>\n",
       "      <td>2937440</td>\n",
       "      <td>26008460</td>\n",
       "      <td>22356</td>\n",
       "      <td>25748</td>\n",
       "      <td>15739</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>102.260</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1143.98</td>\n",
       "      <td>73.95</td>\n",
       "      <td>2057448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>51669716</td>\n",
       "      <td>9550227</td>\n",
       "      <td>13530519</td>\n",
       "      <td>2938429</td>\n",
       "      <td>26019175</td>\n",
       "      <td>22287</td>\n",
       "      <td>25953</td>\n",
       "      <td>14720</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>102.750</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1160.34</td>\n",
       "      <td>68.50</td>\n",
       "      <td>2057448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>51667688</td>\n",
       "      <td>9542256</td>\n",
       "      <td>13542284</td>\n",
       "      <td>2941795</td>\n",
       "      <td>26026335</td>\n",
       "      <td>21893</td>\n",
       "      <td>25674</td>\n",
       "      <td>13733</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>103.170</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1169.54</td>\n",
       "      <td>75.03</td>\n",
       "      <td>2057448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>51662290</td>\n",
       "      <td>9532428</td>\n",
       "      <td>13549577</td>\n",
       "      <td>2945009</td>\n",
       "      <td>26027014</td>\n",
       "      <td>20741</td>\n",
       "      <td>27775</td>\n",
       "      <td>15203</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.4</td>\n",
       "      <td>103.350</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1182.82</td>\n",
       "      <td>83.57</td>\n",
       "      <td>2057448.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>51652704</td>\n",
       "      <td>9520880</td>\n",
       "      <td>13557973</td>\n",
       "      <td>2946319</td>\n",
       "      <td>26025172</td>\n",
       "      <td>19793</td>\n",
       "      <td>28363</td>\n",
       "      <td>17088</td>\n",
       "      <td>...</td>\n",
       "      <td>8.8</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>103.870</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1182.91</td>\n",
       "      <td>66.18</td>\n",
       "      <td>2057448.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>431 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  합계출산율    국내 총인구     서울인구     경기도인구    인천시인구    수도권 인구  \\\n",
       "0   1986-01-01   1.58  41265113  9798542   5075449  1441131  16315122   \n",
       "1   1986-02-01   1.58  41265113  9798542   5075449  1441131  16315122   \n",
       "2   1986-03-01   1.58  41265113  9798542   5075449  1441131  16315122   \n",
       "3   1986-04-01   1.58  41265113  9798542   5075449  1441131  16315122   \n",
       "4   1986-05-01   1.58  41265113  9798542   5075449  1441131  16315122   \n",
       "..         ...    ...       ...      ...       ...      ...       ...   \n",
       "426 2021-07-01   0.81  51671569  9558153  13512867  2937440  26008460   \n",
       "427 2021-08-01   0.81  51669716  9550227  13530519  2938429  26019175   \n",
       "428 2021-09-01   0.81  51667688  9542256  13542284  2941795  26026335   \n",
       "429 2021-10-01   0.81  51662290  9532428  13549577  2945009  26027014   \n",
       "430 2021-11-01   0.81  51652704  9520880  13557973  2946319  26025172   \n",
       "\n",
       "     신생아 출산인구   사망인구   혼인건수  ...  - 농축수산물  - 공업제품  - 집세  - 공공서비스  - 개인서비스  \\\n",
       "0       69708  22924  36162  ...      5.4     2.3   4.7      2.3      4.5   \n",
       "1       73659  20258  36522  ...      5.3     2.2   4.5      2.3      4.5   \n",
       "2       48593  22144  38380  ...      5.8     2.2   4.5      2.1      5.2   \n",
       "3       47880  20892  36333  ...      5.4     1.3   4.3      2.6      5.1   \n",
       "4       47800  18495  37520  ...      4.7     1.7   4.0      2.4      5.1   \n",
       "..        ...    ...    ...  ...      ...     ...   ...      ...      ...   \n",
       "426     22356  25748  15739  ...     11.7     1.1   1.1      0.3      2.3   \n",
       "427     22287  25953  14720  ...     10.9     1.3   1.2      0.4      2.3   \n",
       "428     21893  25674  13733  ...     10.0     1.5   1.2      0.4      2.4   \n",
       "429     20741  27775  15203  ...      8.9     1.8   1.3      0.9      2.4   \n",
       "430     19793  28363  17088  ...      8.8     2.1   1.3      1.0      2.5   \n",
       "\n",
       "     소비자 물가 지수    금리  원 달러 환율  국제유가(WTI, 달러)      국내총생산  \n",
       "0       30.117  9.70   890.53          19.66   102985.8  \n",
       "1       30.293  9.70   886.39          13.23   102985.8  \n",
       "2       30.353  9.70   884.37          10.40   102985.8  \n",
       "3       30.323  9.70   885.60          13.34   102985.8  \n",
       "4       30.501  9.70   887.21          14.25   102985.8  \n",
       "..         ...   ...      ...            ...        ...  \n",
       "426    102.260  0.50  1143.98          73.95  2057448.0  \n",
       "427    102.750  0.75  1160.34          68.50  2057448.0  \n",
       "428    103.170  0.75  1169.54          75.03  2057448.0  \n",
       "429    103.350  0.75  1182.82          83.57  2057448.0  \n",
       "430    103.870  1.00  1182.91          66.18  2057448.0  \n",
       "\n",
       "[431 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(training_data,k,test_size=0.2):\n",
    "    scaler = MinMaxScaler()\n",
    "    training_data = scaler.fit_transform(training_data.to_numpy()[:,1::])\n",
    "    p = []\n",
    "    for i in range(k):\n",
    "        idx = rand.randint(0, 100)\n",
    "        X = training_data[idx:idx+100].reshape(100, 1, 21)\n",
    "        y = training_data[idx+1:idx+1+100]\n",
    "        X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "        p.append([X_train,X_test,y_train,y_test])\n",
    "\n",
    "    return p\n",
    "p = make_dataset(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(my_model, self).__init__()\n",
    "        self.d1 = tf.keras.layers.LSTM(units=128, activation='tanh', return_sequences=True)\n",
    "        self.d2 = tf.keras.layers.LSTM(units=128, activation='tanh', return_sequences=True)\n",
    "        self.d3 = tf.keras.layers.LSTM(units=128, activation='tanh', return_sequences=False)\n",
    "        self.d4 = tf.keras.layers.Dense(units=1, activation='linear')\n",
    "    def call(self, inputs):\n",
    "        x = self.d1(inputs)\n",
    "        x = self.d2(x)\n",
    "        x = self.d3(x)\n",
    "        x = self.d4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = my_model()\n",
    "model_2 = my_model()\n",
    "model_3 = my_model()\n",
    "model_4 = my_model()\n",
    "model_5 = my_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [model_1,model_2,model_3,model_4,model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    model_list[i].build(input_shape=(1, 1, 21))\n",
    "    model_list[i].compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "2/2 [==============================] - 13s 2s/step - loss: 0.2465 - val_loss: 0.2469\n",
      "Epoch 2/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.2308 - val_loss: 0.2271\n",
      "Epoch 3/5000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.2140 - val_loss: 0.2032\n",
      "Epoch 4/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1933 - val_loss: 0.1739\n",
      "Epoch 5/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1687 - val_loss: 0.1395\n",
      "Epoch 6/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1408 - val_loss: 0.1025\n",
      "Epoch 7/5000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.1119 - val_loss: 0.0713\n",
      "Epoch 8/5000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.0901 - val_loss: 0.0625\n",
      "Epoch 9/5000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0892 - val_loss: 0.0783\n",
      "Epoch 10/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.1042 - val_loss: 0.0786\n",
      "Epoch 11/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1032 - val_loss: 0.0672\n",
      "Epoch 12/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0923 - val_loss: 0.0617\n",
      "Epoch 13/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0863 - val_loss: 0.0637\n",
      "Epoch 14/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0864 - val_loss: 0.0681\n",
      "Epoch 15/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0891 - val_loss: 0.0711\n",
      "Epoch 16/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0909 - val_loss: 0.0713\n",
      "Epoch 17/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0908 - val_loss: 0.0691\n",
      "Epoch 18/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0893 - val_loss: 0.0658\n",
      "Epoch 19/5000\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 0.0873 - val_loss: 0.0631\n",
      "Epoch 20/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0860 - val_loss: 0.0617\n",
      "Epoch 21/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0859 - val_loss: 0.0617\n",
      "Epoch 22/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0867 - val_loss: 0.0620\n",
      "Epoch 22: early stopping\n",
      "Epoch 1/5000\n",
      "2/2 [==============================] - 9s 2s/step - loss: 0.2444 - val_loss: 0.2195\n",
      "Epoch 2/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2276 - val_loss: 0.2011\n",
      "Epoch 3/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2085 - val_loss: 0.1790\n",
      "Epoch 4/5000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1857 - val_loss: 0.1518\n",
      "Epoch 5/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1578 - val_loss: 0.1192\n",
      "Epoch 6/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1246 - val_loss: 0.0833\n",
      "Epoch 7/5000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.0888 - val_loss: 0.0506\n",
      "Epoch 8/5000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0576 - val_loss: 0.0362\n",
      "Epoch 9/5000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.0479 - val_loss: 0.0522\n",
      "Epoch 10/5000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0662 - val_loss: 0.0587\n",
      "Epoch 11/5000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0706 - val_loss: 0.0464\n",
      "Epoch 12/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0571 - val_loss: 0.0372\n",
      "Epoch 13/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0478 - val_loss: 0.0368\n",
      "Epoch 14/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0472 - val_loss: 0.0404\n",
      "Epoch 15/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0503 - val_loss: 0.0432\n",
      "Epoch 16/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0528 - val_loss: 0.0436\n",
      "Epoch 17/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0530 - val_loss: 0.0419\n",
      "Epoch 18/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0512 - val_loss: 0.0392\n",
      "Epoch 18: early stopping\n",
      "Epoch 1/5000\n",
      "2/2 [==============================] - 8s 2s/step - loss: 0.2481 - val_loss: 0.2235\n",
      "Epoch 2/5000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2304 - val_loss: 0.2055\n",
      "Epoch 3/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2112 - val_loss: 0.1840\n",
      "Epoch 4/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.1883 - val_loss: 0.1577\n",
      "Epoch 5/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.1600 - val_loss: 0.1260\n",
      "Epoch 6/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1275 - val_loss: 0.0904\n",
      "Epoch 7/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0916 - val_loss: 0.0561\n",
      "Epoch 8/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0599 - val_loss: 0.0361\n",
      "Epoch 9/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0490 - val_loss: 0.0443\n",
      "Epoch 10/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0671 - val_loss: 0.0509\n",
      "Epoch 11/5000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.0733 - val_loss: 0.0420\n",
      "Epoch 12/5000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0600 - val_loss: 0.0358\n",
      "Epoch 13/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0497 - val_loss: 0.0373\n",
      "Epoch 14/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0486 - val_loss: 0.0420\n",
      "Epoch 15/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0516 - val_loss: 0.0454\n",
      "Epoch 16/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0542 - val_loss: 0.0461\n",
      "Epoch 17/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0546 - val_loss: 0.0444\n",
      "Epoch 18/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0531 - val_loss: 0.0412\n",
      "Epoch 19/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0506 - val_loss: 0.0381\n",
      "Epoch 20/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0486 - val_loss: 0.0361\n",
      "Epoch 21/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0480 - val_loss: 0.0355\n",
      "Epoch 22/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0488 - val_loss: 0.0357\n",
      "Epoch 23/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0496 - val_loss: 0.0358\n",
      "Epoch 24/5000\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 0.0496 - val_loss: 0.0356\n",
      "Epoch 25/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0488 - val_loss: 0.0356\n",
      "Epoch 26/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0482 - val_loss: 0.0361\n",
      "Epoch 27/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0480 - val_loss: 0.0369\n",
      "Epoch 28/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0482 - val_loss: 0.0373\n",
      "Epoch 29/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0484 - val_loss: 0.0374\n",
      "Epoch 30/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0484 - val_loss: 0.0370\n",
      "Epoch 31/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0482 - val_loss: 0.0364\n",
      "Epoch 31: early stopping\n",
      "Epoch 1/5000\n",
      "2/2 [==============================] - 10s 2s/step - loss: 0.2577 - val_loss: 0.2379\n",
      "Epoch 2/5000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.2427 - val_loss: 0.2208\n",
      "Epoch 3/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2264 - val_loss: 0.2004\n",
      "Epoch 4/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.2071 - val_loss: 0.1755\n",
      "Epoch 5/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1835 - val_loss: 0.1456\n",
      "Epoch 6/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1562 - val_loss: 0.1118\n",
      "Epoch 7/5000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.1262 - val_loss: 0.0789\n",
      "Epoch 8/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0996 - val_loss: 0.0586\n",
      "Epoch 9/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0873 - val_loss: 0.0654\n",
      "Epoch 10/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0995 - val_loss: 0.0750\n",
      "Epoch 11/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.1082 - val_loss: 0.0672\n",
      "Epoch 12/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0989 - val_loss: 0.0588\n",
      "Epoch 13/5000\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 0.0893 - val_loss: 0.0581\n",
      "Epoch 14/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0869 - val_loss: 0.0619\n",
      "Epoch 15/5000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0890 - val_loss: 0.0656\n",
      "Epoch 16/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0913 - val_loss: 0.0669\n",
      "Epoch 17/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0921 - val_loss: 0.0659\n",
      "Epoch 18/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0912 - val_loss: 0.0632\n",
      "Epoch 19/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0894 - val_loss: 0.0602\n",
      "Epoch 20/5000\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0876 - val_loss: 0.0582\n",
      "Epoch 21/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0869 - val_loss: 0.0576\n",
      "Epoch 22/5000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.0873 - val_loss: 0.0578\n",
      "Epoch 23/5000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.0881 - val_loss: 0.0579\n",
      "Epoch 24/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0882 - val_loss: 0.0577\n",
      "Epoch 25/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0877 - val_loss: 0.0576\n",
      "Epoch 26/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0871 - val_loss: 0.0579\n",
      "Epoch 27/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0868 - val_loss: 0.0585\n",
      "Epoch 28/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0870 - val_loss: 0.0591\n",
      "Epoch 29/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0872 - val_loss: 0.0592\n",
      "Epoch 30/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0873 - val_loss: 0.0590\n",
      "Epoch 31/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0871 - val_loss: 0.0585\n",
      "Epoch 31: early stopping\n",
      "Epoch 1/5000\n",
      "2/2 [==============================] - 9s 2s/step - loss: 0.2802 - val_loss: 0.2527\n",
      "Epoch 2/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.2631 - val_loss: 0.2333\n",
      "Epoch 3/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2444 - val_loss: 0.2099\n",
      "Epoch 4/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.2217 - val_loss: 0.1809\n",
      "Epoch 5/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.1942 - val_loss: 0.1457\n",
      "Epoch 6/5000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1614 - val_loss: 0.1057\n",
      "Epoch 7/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1252 - val_loss: 0.0664\n",
      "Epoch 8/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0934 - val_loss: 0.0419\n",
      "Epoch 9/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.0798 - val_loss: 0.0492\n",
      "Epoch 10/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0951 - val_loss: 0.0580\n",
      "Epoch 11/5000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.1036 - val_loss: 0.0488\n",
      "Epoch 12/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0918 - val_loss: 0.0412\n",
      "Epoch 13/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0810 - val_loss: 0.0424\n",
      "Epoch 14/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0791 - val_loss: 0.0477\n",
      "Epoch 15/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0818 - val_loss: 0.0522\n",
      "Epoch 16/5000\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 0.0847 - val_loss: 0.0535\n",
      "Epoch 17/5000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.0853 - val_loss: 0.0517\n",
      "Epoch 18/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0839 - val_loss: 0.0481\n",
      "Epoch 19/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0815 - val_loss: 0.0443\n",
      "Epoch 20/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0794 - val_loss: 0.0417\n",
      "Epoch 21/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0788 - val_loss: 0.0408\n",
      "Epoch 22/5000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.0796 - val_loss: 0.0409\n",
      "Epoch 23/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0804 - val_loss: 0.0409\n",
      "Epoch 24/5000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.0804 - val_loss: 0.0408\n",
      "Epoch 25/5000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.0796 - val_loss: 0.0411\n",
      "Epoch 26/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0789 - val_loss: 0.0420\n",
      "Epoch 27/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0788 - val_loss: 0.0430\n",
      "Epoch 28/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0791 - val_loss: 0.0437\n",
      "Epoch 29/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0793 - val_loss: 0.0436\n",
      "Epoch 30/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0792 - val_loss: 0.0431\n",
      "Epoch 31/5000\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 0.0790 - val_loss: 0.0423\n",
      "Epoch 32/5000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.0788 - val_loss: 0.0417\n",
      "Epoch 33/5000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.0788 - val_loss: 0.0413\n",
      "Epoch 34/5000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.0789 - val_loss: 0.0412\n",
      "Epoch 34: early stopping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, model in enumerate(model_list):\n",
    "    X_train, X_test, y_train, y_test = p[i][0], p[i][1], p[i][2], p[i][3]\n",
    "    hist = model.fit(X_train, y_train, epochs=5000, batch_size=64, validation_data=(X_test, y_test), verbose=1, \n",
    "                callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net0\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net0\\assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses, lstm_cell_5_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net1\\assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses, lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net2\\assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn, lstm_cell_10_layer_call_and_return_conditional_losses, lstm_cell_11_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net3\\assets\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_12_layer_call_fn, lstm_cell_12_layer_call_and_return_conditional_losses, lstm_cell_13_layer_call_fn, lstm_cell_13_layer_call_and_return_conditional_losses, lstm_cell_14_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/action_net4\\assets\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    model_list[i].save('./model/action_net{0}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
